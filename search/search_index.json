{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Corpora A Dataset Studio for the Digital Humanities What is Corpora? Corpora is Digital Humanities (DH) infrastructure\u2014it's intended to be run by academic centers, libraries, or project teams, and its purpose is to greatly accelerate the development of DH projects by serving as a database, a REST API, a data collection/curation interface, and a Python-powered asynchronous task queue all wrapped into one. Corpora's name is the plural for \"corpus\" because it happily manages multiple, disparate datasets for a variety of different projects. On the same instance of Corpora, you can host one project that contains millions of bibliographic metadata records and another than contains hundreds of thousands of geolocated annotations for images. The limitations for what you can host with Corpora are determined by your hardware, not the affordances of the software. Among Corpora's affordances is a web-based interface for defining a data schema that is expected to evolve over time. This is because it is extremely rare to define a perfect schema for appropriately keeping track of project data on the first attempt. Projects happen iteratively, and so data schemas should be as effortlessly maleable as possible. Once a schema has been defined for a project, Corpora dynamically generates web forms for collaborators to enter the data. It also dynamically generates a read-only REST API for third party applications to query in order to provide, for instance, a public, front-facing website for that project. Corpora's web interface makes use of this REST API to allow scholars to search, sort, and explore the connections between their datapoints as they go about building their project. Finally, Corpora is built to cater to a wide range of users, from scholars with little technical expertise to data wranglers who want to leverage the power of Corpora's built-in iPython notebooks. Corpora also integrates a fully asynchronous job queue allowing power users to run long-running jobs (such as optical character recognition, natural language processing, TEI XML ingestion, etc.) on thousands of records at once. Defining Terms Corpus In Corpora, a \u201ccorpus\u201d is a dataset particular to an individual project. The most approachable metaphor is to think of a corpus as a collection of spreadsheets, each containing data about a different kind of thing (like books, authors, themes, etc), but ultimately all related somehow under the rubric of a project (like a project that collects metadata about literature produced in the 1890\u2019s in London). Unlike a collection of spreadsheets, however, data living in a corpus in Corpora has certain features baked in, like the ability for multiple users to log in and collect that data, the ability to sort and perform full-text searches on the data, the ability for an administrator (or corpus \u201cEditor\u201d) to add new fields for data on-the-fly, the ability to visualize the data in a network graph, and access to an API for programmers or researchers to computationally access that data, whether to perform analyses or to serve up a public-facing frontend website. Also, whereas a spreadsheet that is accessed via Excel or Google Sheets has a hard limit in terms of the amount of data you can usefully store, Corpora is built to scale up to hold millions of records while preserving all functionality. Content Type In Corpora, if a corpus is like a database, then a \u201ccontent type\u201d is like a table in that database. Or to go with the corpus-as-collection-of-spreadsheets metaphor where each spreadsheet is keeping track of a particular thing (like books, authors, themes, etc.), a content type is like one of those spreadsheets. In essence, a content type allows you to store data about a particular thing by allowing you to define it as having a collection of fields (or columns in that spreadsheet), such as, in the case of a book, fields for things like the author, title, publication date, etc. Field A field belongs to a content type, and using the spreadsheet metaphor, it\u2019s like a column in that spreadsheet in that it keeps track of a particular kind of data. So if our content type were a Book, then some common fields to describe that book would be things like author, title, publication date, etc. When creating fields for a content type, there are several types of field you can choose from, each capable of storing different kinds of data and providing different kinds of functionality. The table below lists each field type: Type Description Example Values Sortable? Supported Searches Keyword For storing relatively small (usually textual) values. If you were creating a content type for keeping track of people, you might, for instance, use a keyword field for keeping track of gender. male, female, transgender, gender neutral, non-binary, agender, pangender, genderqueer, two-spirit, third gender Yes Exact on full value Text For storing medium sized text content. Maximum size here depends on how Elasticsearch is configured, but approximately a printed page\u2019s worth of text. \u201cThis is the way the world ends / This is the way the world ends / This is the way the world ends / Not with a bang but a whimper.\u201d Yes Exact on full value, match, term, phrase, wildcard Large Text For storing a large amount of text content, such as the full text of a novel. \u201cCall me Ishmael\u2026\u201d No Match, term, phrase, wildcard HTML For storing a large amount of HTML/XML content, such as a full web page or the TEI encoded text of a novel. Searches for content with tags stripped out. <div><b>This</b> is <i>hypertext</i>!</div> No Match, term, phrase, wildcard Number Whole numbers, positive and negative. 42 Yes Exact on full value, range Decimal Floating point (decimal) numbers, positive and negative. 3.14159 Yes Exact on full value, range Boolean True of false. True Yes Exact Date A date and time. July 2nd, 1964 Yes Exact on full value, range Timespan For storing start dates and optional end dates to represent a range of time. Also features an \"uncertain\" flag that can be set, as well as a granularity value of either Day , Month , or Year . Start: January 1st, 700; End: December 31st, 1050; Uncertain: True; Granularity: Year. (An example of an uncertain publication date between the 8th and early 11th century) Yes Exact on start or end values, range File A digital file uploaded into the corpus file structure. dissertation.docx No On file path: Exact on full value, match, term, phrase, wildcard Git Repo The branch of a git repository, cloned into the file structure of the content to which it belongs. Each corpus also has a \"repos\" property which is a dictionary where the name of the repo is the key and the repo object is its value. https://github.com/bptarpley/corpora.git No None IIIF Image A IIIF image identifier for use with the IIIF image API http://iiif.some.org/iiif/2/cervantes%2F266%2F2001-Beijing-OctoberArts-01-013-f.jpg Yes Exact on full value Geo Point A longitude and latitude. -87.623713,41.879584 No Exact on full value, range Cross Reference A reference to another piece of content with a specified content type. Content of the type Book Yes Each referenced content can be searched by label, but also by any fields belonging to that content type that are marked as being \u201cin lists.\u201d All field types can be configured to hold multiple values, and you can specify whether a given field should have unique values, be full-text indexed, etc. Content In Corpora, \u201ccontent\u201d refers to a specific instance of data structured according to the various fields belonging to its content type. In the spreadsheet metaphor, content is like a row in the spreadsheet referencing a specific item in the list of similar items. Scholar A scholar is simply a user in Corpora. Scholars can view all open-access corpora, and otherwise may be granted either \u201cViewer\u201d or \u201cEditor\u201d privileges for a corpus. Viewer privileges allow scholars to see/search/explore content. Editor privileges allow them to create content, modify the content type schema, run predefined tasks, and run the Corpus Notebook. Scholars can also be granted Admin privileges, allowing them to create a new corpus, manage other scholars, and assume Editor privileges on all corpora. Task A task in Corpora is a predefined algorithm/process that can be run on a piece of content (or on your entire corpus). For example, a task named \u201cOCR with Tesseract 5\u201d might be run on content of the Document content type. Only scholars with Editor or Admin privileges can run tasks. While tasks can be launched using the Corpora web interface, tasks run asynchronously (independently) of the web interface and can take as much time as they need in order to run. The progress of a task is tracked using the web interface, and most tasks also generate a report that can be read during execution and after it has completed. Working with Corpora The following is a guide to working with the Corpora web application to create a corpus, define content types, collect and explore your data, perform searches, and run tasks, among other things. Creating a Corpus To create a corpus, you must be logged in as a scholar with Admin privileges. Assuming the domain/hostname for your instance of Corpora is mycorpora.org, you\u2019d then visit the home/root of that website to see a listing of any existing corpora, i.e. https://mycorpora.org/ When you scroll to the bottom of the page, you\u2019ll see a series of buttons\u2013click on the \u201cNew Corpus\u201d button. In the form provided, provide the following information: Name : A title for the corpus, usually an acronym or a short phrase. Description : A longer description for the purpose of distinguishing the corpus. Open Access : When this checkbox is checked, users will be able to see your content and use the various content API\u2019s to search/query your corpus without having to log into Corpora. Leaving this checkbox unchecked will mean that users will have to log in and have explicit permissions in order to see your content. Any external computers wishing to access the content API\u2019s will also need to have a scholar account with their IP addresses registered, and will also need to use an authentication token. Include default content types from Document plugin : If this box is checked, three content types from the Document plugin will be added to your corpus upon creation: Document , which is structured to support paged artifacts that can be cataloged according to the FRBR standard (work, expression, manifestation, and item); TranscriptionProject , which organizes pages of transcribed text; and Transcription , which represents a page of transcribed text. These default content types would allow a user the ability to upload document pages (via PDF, for instance) and transcribe them using a special transcription GUI. Click the \u201cCreate\u201d button to proceed. Once the corpus has been created, you\u2019ll still be on the home screen showing the corpora available to you, but should now see your new corpus listed among them. Click on the name of your corpus to access your corpus\u2019 dashboard. The Corpus Dashboard Every corpus is assigned an alphanumeric, \u201cuniversally unique\u201d ID upon creation. It will look something like this: 645e8d37ef6052ecc78e06b7 . To visit your corpus\u2019 dashboard, you can either log into the Corpora home page and click on your corpus\u2019 name in the list of available corpora, or you can visit the URL for your corpus, which will always look like [corpora URL]/corpus/[corpus ID]/ . Assuming your corpora URL is https://mycorpora.org and your corpus ID is 645e8d37ef6052ecc78e06b7 , that URL would be: https://mycorpora.org/corpus/645e8d37ef6052ecc78e06b7/ . The corpus dashboard has several tabs. If this is an open access corpus or if you have Viewer privileges, you\u2019ll see the Content, Views, and Metadata tabs. If you\u2019re also either an Editor for this corpus or an Admin for the Corpora instance, you\u2019ll also see the \u201cAdmin\u201d tab. When on the Content tab (the default, activated tab upon visiting the dashboard page), you\u2019ll see links to all the content types listed in the gray box beneath the tab bar. These links are shortcuts to the tabular listing of all the content stored for that content type. All newly created corpora will have the following content types created already: Document, Transcription Project, and Transcription. This suite of content types come from the document plugin, which is the only plugin installed by default in Corpora. The corpus dashboard provides a large amount of functionality that is best broken out and discussed in subsequent sections of documentation. Creating Content Types Below is a video for deleting the default content types that come with a blank corpus, creating some simple content types, changing their labels, and then creating some content. For more detailed instructions, see below the video. To create a content type , you must be logged in as a scholar that is either an Editor for the corpus in question or is an Admin for the instance of Corpora, and must visit the dashboard for that corpus. Once there, click on the \u201cAdmin\u201d tab: The admin tab grants you access to much of the advanced functionality of Corpora, such as running tasks, launching the corpus iPython notebook, and deleting the corpus. Of interest for our purposes now, however, is the Content Type Manager, which can be found by scrolling down: To create a new content type, click the orange \u201cNew Content Type\u201d button on the bottom left. You\u2019ll then see the following form: In the Create Content Type form, you\u2019ll need to provide the following information. Name : This is the name of your content type that will be used to refer to it in various contexts, including a suite of URL\u2019s to access the new content creation form or to query for content via the API. For this reason, the name of a content type can\u2019t contain any special characters or spaces. If your content type is best referred to as a phrase rather than a single word (like \u201cPrinting Press\u201d), it\u2019s recommended that you type the phrase out normally (with a space) in the Name field and then hit the \u201ctab\u201d key to move down to the Plural Name field. Corpora will automatically reformat the phrase you typed (\u201cPrinting Press\u201d) to the camel-cased version of that phrase (\u201cPrintingPress\u201d). Plural Name : This will be used to refer to your content type in various user interface elements, such as the Content tab on the corpus dashboard. This field can contain spaces, so you could use something like \u201cPrinting Presses\u201d as the value for this field. Include in Navigation : This checkbox controls whether the content type will appear on the Content tab of the corpus dashboard, allowing users to view/search its content in a tabular format. If a given content type is not really intended to keep track of data relevant for scholars to see (if, for instance, it\u2019s just a content type used for computational purposes behind the scenes), uncheck this box. Index Labels for Autocompletion : Checking this box will cause the Elasticsearch index to store autocomplete information for the label of each piece of content, which you can read more about below. If you\u2019re not sure you need this, leave it unchecked as it will consume more memory, especially if you\u2019ll be storing millions of records for this content type. Once you\u2019ve filled the form out, click \u201cSave.\u201d You will then see your content type listed in the content type manager. Important note : While your choices have been saved by the user interface, they have not at this point been saved to the database. Closing your browser window or visiting another page at this point will cause you to lose your progress. Adding Fields to Your Content Type In order for a content type to be useful, you must now add fields to it. To do so, click on the name of your new content type to expand out and gain access to the \u201cfields\u201d table: Click the orange \u201cNew Field\u201d button, which will bring up the \u201cCreate New Field\u201d form: To then create a field, you\u2019ll need to provide the following information: Name : This should be the unique name of the field in a \u201csluggified\u201d format (all lowercase with no spaces or special characters other than underscores). This name is how the computer will refer to your field, how you can refer to it using the Python corpus API , and how it will appear in the JSON representation of your data in the various API\u2019s automatically created by Corpora. Label : This is the human readable label for your field, and will be used as the headers for columns in the tabular representation of your content and as the labels for field values in the create/edit/view pages automatically created by Corpora. You may use uppercase letters, spaces, and punctuation marks for field labels. Type : This is the datatype for your field, and you can read about the various types of field here . The type of field will dictate the kind of data you can store in this field, as well as the various ways in which you can search and query your data. Include in Lists : This checkbox determines whether the data stored in this field will be available in the Elasticsearch index, which provides Corpora\u2019s search capacity. If, for instance, you were only storing computational data that doesn\u2019t need to be searched, you can uncheck this box so that the data doesn\u2019t take up too much memory. Indexed : This checkbox determines whether an index for this field will be created in the MongoDB collection used to store your data. Because data also lives in Elasticsearch by default, and because Elasticsearch already provides highly efficient indexes for your fields out-of-the-box, you almost never need to check this box. If, however, you plan on storing millions of records and you plan on accessing them via MongoDB using searches that key off of this field, you may want to check this box. Note, however, that indexing every field in a content type almost never makes sense, and generally defeats the purpose of creating an index at all. Unique : Checking this box will cause MongoDB to enforce a uniqueness constraint on data stored in this field. This means that each value stored in the field will need to be unique. Attempting to save an already existing value to this field will cause the save process to fail. Multiple : Checking this box will allow you to save multiple values of the specified type to this same field. All field types support this functionality, and you\u2019re able to specify those values on the automatically generated content creation/edit form. You\u2019re also able to add multiple values to this field using the python API by treating the field as a list (or array). Once you\u2019ve made the appropriate choices to define your field, click the orange \u201cCreate\u201d button to tentatively add the field to your content type. The word tentatively is used here to highlight the fact that your content type still isn\u2019t saved/created at this point , allowing you the opportunity to continue adding fields until the content type is fully defined. When you are satisfied with how your content types(s) and fields are defined, you must save your content type schema by scrolling down below the list of content types and clicking the orange \u201cSave Changes\u201d button on the bottom right (in the same horizontal space as the \u201cNew Content Type\u201d button): Once you click the save button, the page refreshes and an asynchronous job is launched whose task is to reconfigure the backend data structures according to your new Content Type schema. When an asynchronous job like this completes, you'll see a small alert appear on the bottom-right of the page letting you know you should probably refresh the page before proceeding: It is very important to go ahead and perform that refresh, especially if you are continuing to build out your Content Type schema--you want to make sure the changes you just made are reflected on the page before making further changes. Creating Content Once content types have been created you\u2019re given the opportunity to create content according to your content type schema via the corpus dashboard, specifically on the default \u201cContent\u201d tab. Once there, scroll down to the tabular representation of your specific content type and click the orange \u201cCreate\u201d button: You\u2019ll then be taken to an automatically generated web form allowing you to assign values to each of the fields defined by your content type. Once you\u2019ve entered in the desired values for your fields, click the orange \u201cSave\u201d button at the bottom left: Bulk Creating/Importing Content Often, however, entering the data manually using web forms is inefficient, especially if the data is pre-existing in other formats. There are two main ways of importing existing data. The first is to upload a CSV file containing data for a particular content type to your corpus. You can do this by going to the \u201cMetadata\u201d tab of the corpus dashboard, scrolling down to \u201cCorpus Files,\u201d and clicking the \u201cImport\u201d button. You\u2019ll need to ensure, however, that the columns of the first row in your CSV file contain field names that match the field names of your content type. You can then make use of the CSV plugin which allows you run a task to import the data from that file into your specific content type. The method has limitations, however, in that it does not support importing multi-valued fields or cross referencing between content. The second method is to make use of the Python API for Corpora which allows you to programmatically create content. Often, when going this route, it makes sense to store your source data in an external git repository. You can clone a repository into your corpus by going to the \u201cMetadata\u201d tab on the corpus dashboard, scrolling down to \u201cCorpus Repos,\u201d and clicking the orange \u201cAdd\u201d button. You\u2019ll be able to specify a name for your repo, the git URL for cloning, and the specific branch you\u2019d like to clone. Once you click \u201cSave,\u201d that repository will be immediately cloned and the files in that repo will be available in your corpus\u2019 file structure. Using the Corpus Notebook, for instance, you can then write a script in Python allowing you to pull down changes to your repo, read the updated files, and create any content accordingly. If updating content in this way is a task that will be done frequently, it may make sense to then convert that script into a Corpora task that can be launched at any time via the corpus dashboard. Exporting Content Once content has been created, it may be exported using the tabular representation for its Content Type on the \"Content\" tab of the Corpus Dashboard. To do so, either check the \"check all\" box on the far left of the table header or select individual instances of content using the checkboxes to the left of their respective rows. Note that the \"check all\" box respects search parameters, so feel free to narrow the scope of your data that way first. Once one or more pieces of content have been selected in this way, a \"with selected\" dropdown box should appear right above the column header. From that dropdown, select either \"Export (JSON)\" or \"Export (CSV)\" to begin downloading a streamed export file in either JSON or CSV format. Note that when exporting using the CSV format, any multivalued field values will be \"flattened\" into a single column, with values delimited by the pipe (|) character. Also note that downloading a very large amount of content can take time, as Corpora has to convert each piece of content to the required format before streaming it to the browser client. In general, JSON exports not only represent data more natively (especially for multivalued fields), but are also faster, as less computational resources are required to convert the data.","title":"Getting Started"},{"location":"#corpora","text":"A Dataset Studio for the Digital Humanities","title":"Corpora"},{"location":"#what-is-corpora","text":"Corpora is Digital Humanities (DH) infrastructure\u2014it's intended to be run by academic centers, libraries, or project teams, and its purpose is to greatly accelerate the development of DH projects by serving as a database, a REST API, a data collection/curation interface, and a Python-powered asynchronous task queue all wrapped into one. Corpora's name is the plural for \"corpus\" because it happily manages multiple, disparate datasets for a variety of different projects. On the same instance of Corpora, you can host one project that contains millions of bibliographic metadata records and another than contains hundreds of thousands of geolocated annotations for images. The limitations for what you can host with Corpora are determined by your hardware, not the affordances of the software. Among Corpora's affordances is a web-based interface for defining a data schema that is expected to evolve over time. This is because it is extremely rare to define a perfect schema for appropriately keeping track of project data on the first attempt. Projects happen iteratively, and so data schemas should be as effortlessly maleable as possible. Once a schema has been defined for a project, Corpora dynamically generates web forms for collaborators to enter the data. It also dynamically generates a read-only REST API for third party applications to query in order to provide, for instance, a public, front-facing website for that project. Corpora's web interface makes use of this REST API to allow scholars to search, sort, and explore the connections between their datapoints as they go about building their project. Finally, Corpora is built to cater to a wide range of users, from scholars with little technical expertise to data wranglers who want to leverage the power of Corpora's built-in iPython notebooks. Corpora also integrates a fully asynchronous job queue allowing power users to run long-running jobs (such as optical character recognition, natural language processing, TEI XML ingestion, etc.) on thousands of records at once.","title":"What is Corpora?"},{"location":"#defining-terms","text":"","title":"Defining Terms"},{"location":"#corpus","text":"In Corpora, a \u201ccorpus\u201d is a dataset particular to an individual project. The most approachable metaphor is to think of a corpus as a collection of spreadsheets, each containing data about a different kind of thing (like books, authors, themes, etc), but ultimately all related somehow under the rubric of a project (like a project that collects metadata about literature produced in the 1890\u2019s in London). Unlike a collection of spreadsheets, however, data living in a corpus in Corpora has certain features baked in, like the ability for multiple users to log in and collect that data, the ability to sort and perform full-text searches on the data, the ability for an administrator (or corpus \u201cEditor\u201d) to add new fields for data on-the-fly, the ability to visualize the data in a network graph, and access to an API for programmers or researchers to computationally access that data, whether to perform analyses or to serve up a public-facing frontend website. Also, whereas a spreadsheet that is accessed via Excel or Google Sheets has a hard limit in terms of the amount of data you can usefully store, Corpora is built to scale up to hold millions of records while preserving all functionality.","title":"Corpus"},{"location":"#content-type","text":"In Corpora, if a corpus is like a database, then a \u201ccontent type\u201d is like a table in that database. Or to go with the corpus-as-collection-of-spreadsheets metaphor where each spreadsheet is keeping track of a particular thing (like books, authors, themes, etc.), a content type is like one of those spreadsheets. In essence, a content type allows you to store data about a particular thing by allowing you to define it as having a collection of fields (or columns in that spreadsheet), such as, in the case of a book, fields for things like the author, title, publication date, etc.","title":"Content Type"},{"location":"#field","text":"A field belongs to a content type, and using the spreadsheet metaphor, it\u2019s like a column in that spreadsheet in that it keeps track of a particular kind of data. So if our content type were a Book, then some common fields to describe that book would be things like author, title, publication date, etc. When creating fields for a content type, there are several types of field you can choose from, each capable of storing different kinds of data and providing different kinds of functionality. The table below lists each field type: Type Description Example Values Sortable? Supported Searches Keyword For storing relatively small (usually textual) values. If you were creating a content type for keeping track of people, you might, for instance, use a keyword field for keeping track of gender. male, female, transgender, gender neutral, non-binary, agender, pangender, genderqueer, two-spirit, third gender Yes Exact on full value Text For storing medium sized text content. Maximum size here depends on how Elasticsearch is configured, but approximately a printed page\u2019s worth of text. \u201cThis is the way the world ends / This is the way the world ends / This is the way the world ends / Not with a bang but a whimper.\u201d Yes Exact on full value, match, term, phrase, wildcard Large Text For storing a large amount of text content, such as the full text of a novel. \u201cCall me Ishmael\u2026\u201d No Match, term, phrase, wildcard HTML For storing a large amount of HTML/XML content, such as a full web page or the TEI encoded text of a novel. Searches for content with tags stripped out. <div><b>This</b> is <i>hypertext</i>!</div> No Match, term, phrase, wildcard Number Whole numbers, positive and negative. 42 Yes Exact on full value, range Decimal Floating point (decimal) numbers, positive and negative. 3.14159 Yes Exact on full value, range Boolean True of false. True Yes Exact Date A date and time. July 2nd, 1964 Yes Exact on full value, range Timespan For storing start dates and optional end dates to represent a range of time. Also features an \"uncertain\" flag that can be set, as well as a granularity value of either Day , Month , or Year . Start: January 1st, 700; End: December 31st, 1050; Uncertain: True; Granularity: Year. (An example of an uncertain publication date between the 8th and early 11th century) Yes Exact on start or end values, range File A digital file uploaded into the corpus file structure. dissertation.docx No On file path: Exact on full value, match, term, phrase, wildcard Git Repo The branch of a git repository, cloned into the file structure of the content to which it belongs. Each corpus also has a \"repos\" property which is a dictionary where the name of the repo is the key and the repo object is its value. https://github.com/bptarpley/corpora.git No None IIIF Image A IIIF image identifier for use with the IIIF image API http://iiif.some.org/iiif/2/cervantes%2F266%2F2001-Beijing-OctoberArts-01-013-f.jpg Yes Exact on full value Geo Point A longitude and latitude. -87.623713,41.879584 No Exact on full value, range Cross Reference A reference to another piece of content with a specified content type. Content of the type Book Yes Each referenced content can be searched by label, but also by any fields belonging to that content type that are marked as being \u201cin lists.\u201d All field types can be configured to hold multiple values, and you can specify whether a given field should have unique values, be full-text indexed, etc.","title":"Field"},{"location":"#content","text":"In Corpora, \u201ccontent\u201d refers to a specific instance of data structured according to the various fields belonging to its content type. In the spreadsheet metaphor, content is like a row in the spreadsheet referencing a specific item in the list of similar items.","title":"Content"},{"location":"#scholar","text":"A scholar is simply a user in Corpora. Scholars can view all open-access corpora, and otherwise may be granted either \u201cViewer\u201d or \u201cEditor\u201d privileges for a corpus. Viewer privileges allow scholars to see/search/explore content. Editor privileges allow them to create content, modify the content type schema, run predefined tasks, and run the Corpus Notebook. Scholars can also be granted Admin privileges, allowing them to create a new corpus, manage other scholars, and assume Editor privileges on all corpora.","title":"Scholar"},{"location":"#task","text":"A task in Corpora is a predefined algorithm/process that can be run on a piece of content (or on your entire corpus). For example, a task named \u201cOCR with Tesseract 5\u201d might be run on content of the Document content type. Only scholars with Editor or Admin privileges can run tasks. While tasks can be launched using the Corpora web interface, tasks run asynchronously (independently) of the web interface and can take as much time as they need in order to run. The progress of a task is tracked using the web interface, and most tasks also generate a report that can be read during execution and after it has completed.","title":"Task"},{"location":"#working-with-corpora","text":"The following is a guide to working with the Corpora web application to create a corpus, define content types, collect and explore your data, perform searches, and run tasks, among other things.","title":"Working with Corpora"},{"location":"#creating-a-corpus","text":"To create a corpus, you must be logged in as a scholar with Admin privileges. Assuming the domain/hostname for your instance of Corpora is mycorpora.org, you\u2019d then visit the home/root of that website to see a listing of any existing corpora, i.e. https://mycorpora.org/ When you scroll to the bottom of the page, you\u2019ll see a series of buttons\u2013click on the \u201cNew Corpus\u201d button. In the form provided, provide the following information: Name : A title for the corpus, usually an acronym or a short phrase. Description : A longer description for the purpose of distinguishing the corpus. Open Access : When this checkbox is checked, users will be able to see your content and use the various content API\u2019s to search/query your corpus without having to log into Corpora. Leaving this checkbox unchecked will mean that users will have to log in and have explicit permissions in order to see your content. Any external computers wishing to access the content API\u2019s will also need to have a scholar account with their IP addresses registered, and will also need to use an authentication token. Include default content types from Document plugin : If this box is checked, three content types from the Document plugin will be added to your corpus upon creation: Document , which is structured to support paged artifacts that can be cataloged according to the FRBR standard (work, expression, manifestation, and item); TranscriptionProject , which organizes pages of transcribed text; and Transcription , which represents a page of transcribed text. These default content types would allow a user the ability to upload document pages (via PDF, for instance) and transcribe them using a special transcription GUI. Click the \u201cCreate\u201d button to proceed. Once the corpus has been created, you\u2019ll still be on the home screen showing the corpora available to you, but should now see your new corpus listed among them. Click on the name of your corpus to access your corpus\u2019 dashboard.","title":"Creating a Corpus"},{"location":"#the-corpus-dashboard","text":"Every corpus is assigned an alphanumeric, \u201cuniversally unique\u201d ID upon creation. It will look something like this: 645e8d37ef6052ecc78e06b7 . To visit your corpus\u2019 dashboard, you can either log into the Corpora home page and click on your corpus\u2019 name in the list of available corpora, or you can visit the URL for your corpus, which will always look like [corpora URL]/corpus/[corpus ID]/ . Assuming your corpora URL is https://mycorpora.org and your corpus ID is 645e8d37ef6052ecc78e06b7 , that URL would be: https://mycorpora.org/corpus/645e8d37ef6052ecc78e06b7/ . The corpus dashboard has several tabs. If this is an open access corpus or if you have Viewer privileges, you\u2019ll see the Content, Views, and Metadata tabs. If you\u2019re also either an Editor for this corpus or an Admin for the Corpora instance, you\u2019ll also see the \u201cAdmin\u201d tab. When on the Content tab (the default, activated tab upon visiting the dashboard page), you\u2019ll see links to all the content types listed in the gray box beneath the tab bar. These links are shortcuts to the tabular listing of all the content stored for that content type. All newly created corpora will have the following content types created already: Document, Transcription Project, and Transcription. This suite of content types come from the document plugin, which is the only plugin installed by default in Corpora. The corpus dashboard provides a large amount of functionality that is best broken out and discussed in subsequent sections of documentation.","title":"The Corpus Dashboard"},{"location":"#creating-content-types","text":"Below is a video for deleting the default content types that come with a blank corpus, creating some simple content types, changing their labels, and then creating some content. For more detailed instructions, see below the video. To create a content type , you must be logged in as a scholar that is either an Editor for the corpus in question or is an Admin for the instance of Corpora, and must visit the dashboard for that corpus. Once there, click on the \u201cAdmin\u201d tab: The admin tab grants you access to much of the advanced functionality of Corpora, such as running tasks, launching the corpus iPython notebook, and deleting the corpus. Of interest for our purposes now, however, is the Content Type Manager, which can be found by scrolling down: To create a new content type, click the orange \u201cNew Content Type\u201d button on the bottom left. You\u2019ll then see the following form: In the Create Content Type form, you\u2019ll need to provide the following information. Name : This is the name of your content type that will be used to refer to it in various contexts, including a suite of URL\u2019s to access the new content creation form or to query for content via the API. For this reason, the name of a content type can\u2019t contain any special characters or spaces. If your content type is best referred to as a phrase rather than a single word (like \u201cPrinting Press\u201d), it\u2019s recommended that you type the phrase out normally (with a space) in the Name field and then hit the \u201ctab\u201d key to move down to the Plural Name field. Corpora will automatically reformat the phrase you typed (\u201cPrinting Press\u201d) to the camel-cased version of that phrase (\u201cPrintingPress\u201d). Plural Name : This will be used to refer to your content type in various user interface elements, such as the Content tab on the corpus dashboard. This field can contain spaces, so you could use something like \u201cPrinting Presses\u201d as the value for this field. Include in Navigation : This checkbox controls whether the content type will appear on the Content tab of the corpus dashboard, allowing users to view/search its content in a tabular format. If a given content type is not really intended to keep track of data relevant for scholars to see (if, for instance, it\u2019s just a content type used for computational purposes behind the scenes), uncheck this box. Index Labels for Autocompletion : Checking this box will cause the Elasticsearch index to store autocomplete information for the label of each piece of content, which you can read more about below. If you\u2019re not sure you need this, leave it unchecked as it will consume more memory, especially if you\u2019ll be storing millions of records for this content type. Once you\u2019ve filled the form out, click \u201cSave.\u201d You will then see your content type listed in the content type manager. Important note : While your choices have been saved by the user interface, they have not at this point been saved to the database. Closing your browser window or visiting another page at this point will cause you to lose your progress.","title":"Creating Content Types"},{"location":"#adding-fields-to-your-content-type","text":"In order for a content type to be useful, you must now add fields to it. To do so, click on the name of your new content type to expand out and gain access to the \u201cfields\u201d table: Click the orange \u201cNew Field\u201d button, which will bring up the \u201cCreate New Field\u201d form: To then create a field, you\u2019ll need to provide the following information: Name : This should be the unique name of the field in a \u201csluggified\u201d format (all lowercase with no spaces or special characters other than underscores). This name is how the computer will refer to your field, how you can refer to it using the Python corpus API , and how it will appear in the JSON representation of your data in the various API\u2019s automatically created by Corpora. Label : This is the human readable label for your field, and will be used as the headers for columns in the tabular representation of your content and as the labels for field values in the create/edit/view pages automatically created by Corpora. You may use uppercase letters, spaces, and punctuation marks for field labels. Type : This is the datatype for your field, and you can read about the various types of field here . The type of field will dictate the kind of data you can store in this field, as well as the various ways in which you can search and query your data. Include in Lists : This checkbox determines whether the data stored in this field will be available in the Elasticsearch index, which provides Corpora\u2019s search capacity. If, for instance, you were only storing computational data that doesn\u2019t need to be searched, you can uncheck this box so that the data doesn\u2019t take up too much memory. Indexed : This checkbox determines whether an index for this field will be created in the MongoDB collection used to store your data. Because data also lives in Elasticsearch by default, and because Elasticsearch already provides highly efficient indexes for your fields out-of-the-box, you almost never need to check this box. If, however, you plan on storing millions of records and you plan on accessing them via MongoDB using searches that key off of this field, you may want to check this box. Note, however, that indexing every field in a content type almost never makes sense, and generally defeats the purpose of creating an index at all. Unique : Checking this box will cause MongoDB to enforce a uniqueness constraint on data stored in this field. This means that each value stored in the field will need to be unique. Attempting to save an already existing value to this field will cause the save process to fail. Multiple : Checking this box will allow you to save multiple values of the specified type to this same field. All field types support this functionality, and you\u2019re able to specify those values on the automatically generated content creation/edit form. You\u2019re also able to add multiple values to this field using the python API by treating the field as a list (or array). Once you\u2019ve made the appropriate choices to define your field, click the orange \u201cCreate\u201d button to tentatively add the field to your content type. The word tentatively is used here to highlight the fact that your content type still isn\u2019t saved/created at this point , allowing you the opportunity to continue adding fields until the content type is fully defined. When you are satisfied with how your content types(s) and fields are defined, you must save your content type schema by scrolling down below the list of content types and clicking the orange \u201cSave Changes\u201d button on the bottom right (in the same horizontal space as the \u201cNew Content Type\u201d button): Once you click the save button, the page refreshes and an asynchronous job is launched whose task is to reconfigure the backend data structures according to your new Content Type schema. When an asynchronous job like this completes, you'll see a small alert appear on the bottom-right of the page letting you know you should probably refresh the page before proceeding: It is very important to go ahead and perform that refresh, especially if you are continuing to build out your Content Type schema--you want to make sure the changes you just made are reflected on the page before making further changes.","title":"Adding Fields to Your Content Type"},{"location":"#creating-content","text":"Once content types have been created you\u2019re given the opportunity to create content according to your content type schema via the corpus dashboard, specifically on the default \u201cContent\u201d tab. Once there, scroll down to the tabular representation of your specific content type and click the orange \u201cCreate\u201d button: You\u2019ll then be taken to an automatically generated web form allowing you to assign values to each of the fields defined by your content type. Once you\u2019ve entered in the desired values for your fields, click the orange \u201cSave\u201d button at the bottom left:","title":"Creating Content"},{"location":"#bulk-creatingimporting-content","text":"Often, however, entering the data manually using web forms is inefficient, especially if the data is pre-existing in other formats. There are two main ways of importing existing data. The first is to upload a CSV file containing data for a particular content type to your corpus. You can do this by going to the \u201cMetadata\u201d tab of the corpus dashboard, scrolling down to \u201cCorpus Files,\u201d and clicking the \u201cImport\u201d button. You\u2019ll need to ensure, however, that the columns of the first row in your CSV file contain field names that match the field names of your content type. You can then make use of the CSV plugin which allows you run a task to import the data from that file into your specific content type. The method has limitations, however, in that it does not support importing multi-valued fields or cross referencing between content. The second method is to make use of the Python API for Corpora which allows you to programmatically create content. Often, when going this route, it makes sense to store your source data in an external git repository. You can clone a repository into your corpus by going to the \u201cMetadata\u201d tab on the corpus dashboard, scrolling down to \u201cCorpus Repos,\u201d and clicking the orange \u201cAdd\u201d button. You\u2019ll be able to specify a name for your repo, the git URL for cloning, and the specific branch you\u2019d like to clone. Once you click \u201cSave,\u201d that repository will be immediately cloned and the files in that repo will be available in your corpus\u2019 file structure. Using the Corpus Notebook, for instance, you can then write a script in Python allowing you to pull down changes to your repo, read the updated files, and create any content accordingly. If updating content in this way is a task that will be done frequently, it may make sense to then convert that script into a Corpora task that can be launched at any time via the corpus dashboard.","title":"Bulk Creating/Importing Content"},{"location":"#exporting-content","text":"Once content has been created, it may be exported using the tabular representation for its Content Type on the \"Content\" tab of the Corpus Dashboard. To do so, either check the \"check all\" box on the far left of the table header or select individual instances of content using the checkboxes to the left of their respective rows. Note that the \"check all\" box respects search parameters, so feel free to narrow the scope of your data that way first. Once one or more pieces of content have been selected in this way, a \"with selected\" dropdown box should appear right above the column header. From that dropdown, select either \"Export (JSON)\" or \"Export (CSV)\" to begin downloading a streamed export file in either JSON or CSV format. Note that when exporting using the CSV format, any multivalued field values will be \"flattened\" into a single column, with values delimited by the pipe (|) character. Also note that downloading a very large amount of content can take time, as Corpora has to convert each piece of content to the required format before streaming it to the browser client. In general, JSON exports not only represent data more natively (especially for multivalued fields), but are also faster, as less computational resources are required to convert the data.","title":"Exporting Content"},{"location":"corpus_api/","text":"Corpus API Corpus API This module provides a comprehensive data management system that mirrors database functionality using MongoDB for document storage, Elasticsearch for full-text search, and Neo4j for graph relationships. The core architecture follows a hierarchical structure: Corpus (analogous to a database) Content Types (analogous to tables) Fields (analogous to columns) Content instances (analogous to rows) The system supports: - Dynamic schema definition and modification - Multi-language text analysis (34 languages) - Cross-references between content types - File and media management - Provenance tracking and audit trails - Advanced search with faceting and aggregations - Graph-based relationship queries","title":"Corpus API"},{"location":"corpus_api/#corpus-api","text":"Corpus API This module provides a comprehensive data management system that mirrors database functionality using MongoDB for document storage, Elasticsearch for full-text search, and Neo4j for graph relationships. The core architecture follows a hierarchical structure: Corpus (analogous to a database) Content Types (analogous to tables) Fields (analogous to columns) Content instances (analogous to rows) The system supports: - Dynamic schema definition and modification - Multi-language text analysis (34 languages) - Cross-references between content types - File and media management - Provenance tracking and audit trails - Advanced search with faceting and aggregations - Graph-based relationship queries","title":"Corpus API"},{"location":"deploying/","text":"Deploying Corpora is built from the ground up as a \"Dockerized\" stack of services. In other words, each service lives in its own Docker container, and these containers are orchestrated to provide Corpora's functionality. The following diagram is an attempt to provide a bird's eye view of how these containers are used: Despite this complexity, Corpora was designed in order to make installing it on personal computers or servers as easy as possible thanks to the \"docker compose\" convention of orchestrating containers. Below are instructions for installing Corpora on Mac, Windows, and Linux. Prerequisites Regardless of which operating system you're using, you'll need to install Docker. The easiest way to do this on Mac or Windows is by installing Docker Desktop . For a Linux server, you'll want to install Docker and Docker Compose packages. Instructions for installing on Ubuntu, for instance, can be found here . Once Docker has been installed, you'll want to identify two different places to store files on your system. The first is where you want to clone the Corpora codebase, and the second is where you want to store the data and configuration for your instance of Corpora. These directories can exist wherever you'd like, but for the sake of convenience throughout these instructions, we'll assume you've created the directory /corpora and will be making your two directories in there. If you have Git installed on your machine, the easiest way to acquire the codebase is to clone the Corpora repository : cd /corpora git clone https://github.com/bptarpley/corpora.git codebase The commands above will create the directory /corpora/codebase and pull of the code into that directory. If you don't use git, an alternative is to download the .zip file for Corpora's code and unzip it inside the /corpora directory--for the purpose of these instructions you would then rename the unzipped folder to \"codebase.\" Once you have your codebase directory set up, you'll want to create another directory inside of /corpora called \"data.\" Make note of the path to this second directory (i.e. /corpora/data ), as you'll be needing it for creating a docker-compose.yml file in later steps. Installing and Running on Your Local Machine (Mac or Windows) Once you've installed Docker Desktop and made your two directories according to the instructions above, you'll need to go to the codebase directory and make a copy of the docker-compose.yml.example file, naming it docker-compose.yml . Note: Be sure your copy of the file lives in the same codebase directory as the original. You'll then need to edit that file using any text editor in order to replace any instance of /your/corpora/basedir with the data directory you created, i.e. /corpora/data . Having created and edited your docker-compose.yml file, you'll need to open a terminal window (Mac) or DOS prompt (Windows) and change to the codebase directory, i.e.: cd /corpora/codebase You'll then want to execute the following command to prompt Docker to pull down the necessary images and start running the Corpora stack: docker-compose up -d Note: Docker will only pull down the required images the first time you run that command, but depending on your internet connection, this may take some time. Once it has pulled the images and created each service, Corpora's startup script is set to wait a full minute before initializing--this is to give the MongoDB, Elasticsearch, and Neo4J containers enough time to spin up. If at any time you want to check the logs for the Corpora container, you can do so by executing this command in your terminal/DOS prompt: docker-compose logs -f corpora Eventually, you'll see the following log message letting you know Corpora was initialized properly: --------------------------- CORPORA INITIALIZED --------------------------- Once this happens, it may take another few seconds for the Daphne web server to actually serve the Corpora web application, but you should eventually be able to use the application by going to this URL in your local machine's browser: http://localhost . You should see Corpora's login prompt, and the default admin account's username and password are both simply corpora . To stop Corpora gracefully, you'll want to go back to the terminal/DOS prompt. If you're streaming log messages, you'll want to stop doing so by hitting CTRL+c. You'll then want to run this command: docker-compose down To get it running again, open a terminal/DOS prompt, change to the /corpora/codebase directory, and issue docker-compose up -d . Installing and Running on a Linux Server While the above instructions for running Corpora on a local machine will also work on a Linux server, there are some additional considerations when deploying it to a server that will be primarily accessed remotely. In such a scenario, you will likely have a domain name (like mycorpora.org ) you'll want to use, you'll want to use SSL to secure network traffic, and will probably want to run the Docker stack in Swarm mode rather than using Docker Compose for performance and stability. Using a Custom Domain Name In order for the Corpora web app to respond correctly to web requests at a specific domain, you'll need to add that domain to the comma-delimited list of domain names provided in the CRP_HOSTS environment variable. This can most easily be configured by editing your docker-compose.yml file. You'll want to find the following line under the \"environment\" section of the \"corpora\" service: CRP_HOSTS: localhost Append a comma directly after \"localhost\" and add your own domain name, i.e.: CRP_HOSTS: localhost,mycorpora.org Directory Creation and Ownership While directory creation and ownership are usually automatically setup for you when running Corpora on Docker Desktop, the same cannot be said for running on a Linux server. When running on Linux, you must manually create directories and set ownership inside the data directory you set up, i.e. /corpora/data . Here is a tree based representation of those directories, with the ownership info (UID:GID) of those directories specified next to them in parentheses: - archive (999:0) - conf (1000:1000) - corpora (1000:1000) - iiif (100:100) |_ cache (100:100) |_ temp (100:100) - link (7474:7474) |_ data (7474:7474) |_ logs (7474:7474) - search (1000:0) |_ data (1000:0) |_ logs (1000:0) - static (1000:1000) Running in Swarm Mode Docker Swarm is a technology for orchestrating containers in a highly available, fault-tolerant server environment (it's what Kubernetes uses under the hood). Ideally, a fully-fledged Docker Swarm deployment would run as a cluster of servers with three master nodes and at least a couple of worker nodes, and configuring such a setup is beyond the scope of this documentation. Swarm will run, however, on a single server, and instructions for doing so are provided here for the sake of simplicity. First, make sure you've taken care of the prerequisites, made a copy of docker-compose.yml.example named docker-compose.yml and replaced all instances of /your/corpora/basedir with the path to your data directory, i.e. /corpora/data . Next, you'll want to make sure your server's Docker instance is running in Swarm mode by executing this command: docker swarm init Then, you'll want to create a private network to keep the traffic between Corpora's various services contained: docker network create -d overlay corpora-private Next, you'll want to edit the docker-compose.yml file, adding the following section to each of the services (nginx, corpora, redis, iiif, mongo, neo4j, and elastic): networks: - corpora-private At the bottom of your docker-compose.yml file, you'll then specify a separate \"networks\" section at the root level of indentation: networks: corpora-private: external: true Finally, you can actually deploy the corpora stack as a Swarm service by issuing this command inside the codebase directory: docker stack deploy corpora -c docker-compose.yml You can watch the logs for Corpora running as a Swarm service by issuing this command: docker service logs -f corpora_corpora Note: Occasionally upon starting, Nginx marks the upstream Corpora service as unavailable because it initialized much faster than the Corpora service. If this is the case, you'll see a \"Bad Gateway\" error when trying to access Corpora. To fix this, you'll want to just restart the NGINX service by executing this command: docker service update --force corpora_nginx Once everything's up and running properly, you should be able to navigate to your domain and see the Corpora login prompt. Be sure to change the password (corpora) for the default \"corpora\" user as soon as possible. To stop the Corpora services running in swarm mode, issue this command: docker stack rm corpora Using SSL Regardless of where you terminate your SSL traffic, you'll need to configure the Corpora service properly so it can be aware of the need to use the \"https\" prefix instead of \"http\" when generating internal links. To do so, you'll need to edit the docker-compose.yml file, find the line that reads CRP_USE_SSL: \"no\" and change the \"no\" value to \"yes\".","title":"Deploying"},{"location":"deploying/#deploying","text":"Corpora is built from the ground up as a \"Dockerized\" stack of services. In other words, each service lives in its own Docker container, and these containers are orchestrated to provide Corpora's functionality. The following diagram is an attempt to provide a bird's eye view of how these containers are used: Despite this complexity, Corpora was designed in order to make installing it on personal computers or servers as easy as possible thanks to the \"docker compose\" convention of orchestrating containers. Below are instructions for installing Corpora on Mac, Windows, and Linux.","title":"Deploying"},{"location":"deploying/#prerequisites","text":"Regardless of which operating system you're using, you'll need to install Docker. The easiest way to do this on Mac or Windows is by installing Docker Desktop . For a Linux server, you'll want to install Docker and Docker Compose packages. Instructions for installing on Ubuntu, for instance, can be found here . Once Docker has been installed, you'll want to identify two different places to store files on your system. The first is where you want to clone the Corpora codebase, and the second is where you want to store the data and configuration for your instance of Corpora. These directories can exist wherever you'd like, but for the sake of convenience throughout these instructions, we'll assume you've created the directory /corpora and will be making your two directories in there. If you have Git installed on your machine, the easiest way to acquire the codebase is to clone the Corpora repository : cd /corpora git clone https://github.com/bptarpley/corpora.git codebase The commands above will create the directory /corpora/codebase and pull of the code into that directory. If you don't use git, an alternative is to download the .zip file for Corpora's code and unzip it inside the /corpora directory--for the purpose of these instructions you would then rename the unzipped folder to \"codebase.\" Once you have your codebase directory set up, you'll want to create another directory inside of /corpora called \"data.\" Make note of the path to this second directory (i.e. /corpora/data ), as you'll be needing it for creating a docker-compose.yml file in later steps.","title":"Prerequisites"},{"location":"deploying/#installing-and-running-on-your-local-machine-mac-or-windows","text":"Once you've installed Docker Desktop and made your two directories according to the instructions above, you'll need to go to the codebase directory and make a copy of the docker-compose.yml.example file, naming it docker-compose.yml . Note: Be sure your copy of the file lives in the same codebase directory as the original. You'll then need to edit that file using any text editor in order to replace any instance of /your/corpora/basedir with the data directory you created, i.e. /corpora/data . Having created and edited your docker-compose.yml file, you'll need to open a terminal window (Mac) or DOS prompt (Windows) and change to the codebase directory, i.e.: cd /corpora/codebase You'll then want to execute the following command to prompt Docker to pull down the necessary images and start running the Corpora stack: docker-compose up -d Note: Docker will only pull down the required images the first time you run that command, but depending on your internet connection, this may take some time. Once it has pulled the images and created each service, Corpora's startup script is set to wait a full minute before initializing--this is to give the MongoDB, Elasticsearch, and Neo4J containers enough time to spin up. If at any time you want to check the logs for the Corpora container, you can do so by executing this command in your terminal/DOS prompt: docker-compose logs -f corpora Eventually, you'll see the following log message letting you know Corpora was initialized properly: --------------------------- CORPORA INITIALIZED --------------------------- Once this happens, it may take another few seconds for the Daphne web server to actually serve the Corpora web application, but you should eventually be able to use the application by going to this URL in your local machine's browser: http://localhost . You should see Corpora's login prompt, and the default admin account's username and password are both simply corpora . To stop Corpora gracefully, you'll want to go back to the terminal/DOS prompt. If you're streaming log messages, you'll want to stop doing so by hitting CTRL+c. You'll then want to run this command: docker-compose down To get it running again, open a terminal/DOS prompt, change to the /corpora/codebase directory, and issue docker-compose up -d .","title":"Installing and Running on Your Local Machine (Mac or Windows)"},{"location":"deploying/#installing-and-running-on-a-linux-server","text":"While the above instructions for running Corpora on a local machine will also work on a Linux server, there are some additional considerations when deploying it to a server that will be primarily accessed remotely. In such a scenario, you will likely have a domain name (like mycorpora.org ) you'll want to use, you'll want to use SSL to secure network traffic, and will probably want to run the Docker stack in Swarm mode rather than using Docker Compose for performance and stability.","title":"Installing and Running on a Linux Server"},{"location":"deploying/#using-a-custom-domain-name","text":"In order for the Corpora web app to respond correctly to web requests at a specific domain, you'll need to add that domain to the comma-delimited list of domain names provided in the CRP_HOSTS environment variable. This can most easily be configured by editing your docker-compose.yml file. You'll want to find the following line under the \"environment\" section of the \"corpora\" service: CRP_HOSTS: localhost Append a comma directly after \"localhost\" and add your own domain name, i.e.: CRP_HOSTS: localhost,mycorpora.org","title":"Using a Custom Domain Name"},{"location":"deploying/#directory-creation-and-ownership","text":"While directory creation and ownership are usually automatically setup for you when running Corpora on Docker Desktop, the same cannot be said for running on a Linux server. When running on Linux, you must manually create directories and set ownership inside the data directory you set up, i.e. /corpora/data . Here is a tree based representation of those directories, with the ownership info (UID:GID) of those directories specified next to them in parentheses: - archive (999:0) - conf (1000:1000) - corpora (1000:1000) - iiif (100:100) |_ cache (100:100) |_ temp (100:100) - link (7474:7474) |_ data (7474:7474) |_ logs (7474:7474) - search (1000:0) |_ data (1000:0) |_ logs (1000:0) - static (1000:1000)","title":"Directory Creation and Ownership"},{"location":"deploying/#running-in-swarm-mode","text":"Docker Swarm is a technology for orchestrating containers in a highly available, fault-tolerant server environment (it's what Kubernetes uses under the hood). Ideally, a fully-fledged Docker Swarm deployment would run as a cluster of servers with three master nodes and at least a couple of worker nodes, and configuring such a setup is beyond the scope of this documentation. Swarm will run, however, on a single server, and instructions for doing so are provided here for the sake of simplicity. First, make sure you've taken care of the prerequisites, made a copy of docker-compose.yml.example named docker-compose.yml and replaced all instances of /your/corpora/basedir with the path to your data directory, i.e. /corpora/data . Next, you'll want to make sure your server's Docker instance is running in Swarm mode by executing this command: docker swarm init Then, you'll want to create a private network to keep the traffic between Corpora's various services contained: docker network create -d overlay corpora-private Next, you'll want to edit the docker-compose.yml file, adding the following section to each of the services (nginx, corpora, redis, iiif, mongo, neo4j, and elastic): networks: - corpora-private At the bottom of your docker-compose.yml file, you'll then specify a separate \"networks\" section at the root level of indentation: networks: corpora-private: external: true Finally, you can actually deploy the corpora stack as a Swarm service by issuing this command inside the codebase directory: docker stack deploy corpora -c docker-compose.yml You can watch the logs for Corpora running as a Swarm service by issuing this command: docker service logs -f corpora_corpora Note: Occasionally upon starting, Nginx marks the upstream Corpora service as unavailable because it initialized much faster than the Corpora service. If this is the case, you'll see a \"Bad Gateway\" error when trying to access Corpora. To fix this, you'll want to just restart the NGINX service by executing this command: docker service update --force corpora_nginx Once everything's up and running properly, you should be able to navigate to your domain and see the Corpora login prompt. Be sure to change the password (corpora) for the default \"corpora\" user as soon as possible. To stop the Corpora services running in swarm mode, issue this command: docker stack rm corpora","title":"Running in Swarm Mode"},{"location":"deploying/#using-ssl","text":"Regardless of where you terminate your SSL traffic, you'll need to configure the Corpora service properly so it can be aware of the need to use the \"https\" prefix instead of \"http\" when generating internal links. To do so, you'll need to edit the docker-compose.yml file, find the line that reads CRP_USE_SSL: \"no\" and change the \"no\" value to \"yes\".","title":"Using SSL"},{"location":"developing/","text":"Developing Corpora was built with the DH developer in mind just as much as the DH scholar. It was created as a way to flexibly handle a wide variety of DH projects with minimal effort while also empowering the developer to focus on the more innovative aspects of a given project. Below are detailed the affordances of corpora tailored specifically for developers, listed in order of complexity. Content Type Templates From a developer's perspective, a Content Type in Corpora is a class whose properties and methods are largely defined by the Content Type Manager (essentially a data schema editor) available on the Admin tab of a given corpus. Content Type Templates are the convention Corpora uses to allow developers to control how instances of content get rendered , whether as a textual label, an HTML snippet, a JavaScript component, an XML document, etc. This rendering is done using the Jinja2-style Django template convention , and indeed it's recommended to refer to Django's documentation when editing or creating templates (particularly as a reference for Django's built-in template tags ). Editing Label Templates By way of example, let's consider the following Content Type used to keep track of named entities throughout a corpus of XML documents: There are four fields defined for this Content Type, which means any instance of this content will be an object with at least four properties. If we were to name an instance of this class Entity , then you'd access its four properties like this using pseudocode: Entity.xml_id Entity.entity_type Entity.name Entity.uris There are also always three more hidden properties available for any instance of Content: Entity.id # a unique, alphanumeric identifier Entity.uri # a unique URI for this content which includes its corpus ID Entity.label # a textual representation of the content The label property for any given piece of content in Corpora is generated using the template called \"Label\" which can be edited using the Content Type Manager. To edit the Label template, go to the Admin tab of your corpus, scroll to the Content Type Manager, and expand out the tray for a given Content Type. Scroll to the bottom of that tray, and locate in the footer of the table that lists the fields for your Content Type the dropdown prefixed with the label \"Edit Template.\" Click the \"Go\" button to begin editing the template used by Corpora to generate textual labels for that Content Type: In this particular case, the template for our label looks like this: {{ Entity.name }} ({{ Entity.entity_type }}) When editing a Content Type template in Corpora, the template's \"namespace\" has available to it an instance of the content named the same as the Content Type (i.e. Entity ). Django's templating system has a convention whereby you can dynamically insert the value for an object's property by surrounding it with double curly-braces. So to output the value of your Entity's name property in a template, you'd use: {{ Entity.name }} Notice this is how our template example begins. The rest of the example includes a space, open parenthesis, the output of the value for the Entity's entity_type field, and then a closing parenthesis. Given this template, if our instance of the Entity Content Type had the value \"Maria Edgeworth\" for the field name , and \"PERSON\" for the field entity_type , the textual label for that piece of content would look like this: Maria Edgeworth (PERSON) Django's templating system is powerful, as it also provides affordances for boolean logic in the form of if/else statements. Let's say we want our label template to be a little more sophisticated by having it provide default values for fields that have no value. In this case, we'll leverage Django's built-in {% if ... %} syntax : {% if Entity.name %}{{ Entity.name }}{% else %}Unknown{% endif %} ({% if Entity.entity_type %}{{ Entity.entity_type }}{% else %}UNKNOWN{% endif %}) Using this new template, if the Entity's name property has no value, the string \"Unknown\" will be output. Similarly, if entity_type has no value, \"UNKNOWN\" will be output. Note that when you make changes to a template in Corpora's Content Type Manager, you must click the orange \"Save\" button on the \"Edit Template\" modal, and must also click the orange \"Save Changes\" button in the footer of the Content Type Manager for template changes to be \"committed\" to your data schema. Also note that when the Label template is changed, Corpora automatically fires off a reindexing task for the Content Type in question, as well as for any other Content Types in your corpus that reference the Content Type in question. Depending on how many instances of these Content Types you have in your corpus, this reindexing may take some time. Creating New Templates Beyond specifying how content labels get created, Corpora's Content Type templating system allows you to create almost any kind of web-based representation of your content by allowing you to build a template and choose the appropriate MIME type for that representation. Building off our Entity example, let's say you wanted to create TEI XML representations for entities in your corpus. In the Content Type Manager for your corpus, you'd expand out the tray for your Content Type and, next to the \"Go\" button for editing an existing template, you'd click the \"New Template\" button to bring up the template editor: Give your template a URL-friendly name (no spaces or special characters), provide the content for your template, and choose an appropriate MIME Type (in this case, text/xml so we can serve up XML for the output of this template). In case the image above is too small or blurry, here's the content for this template: <person xml:id=\"{{ Entity.xml_id }}\"> <persName>{{ Entity.name }}</persName> </person> Click the \"Save\" button on the template editing modal, and then click \"Save Changes\" at the bottom of the Content Type Manager. Once this happens, your new template is available to be rendered. Viewing Rendered Templates To view the output of a template for an instance of content, you'll need to construct a URL that follows this convention: [Your Corpora Instance]/corpus/[Corpus ID]/[Content Type]/[Content ID]/?render_template=[Template Name] In this example, let's assume your Corpora instance is hosted at https://mycorpora.org , your Corpus ID is 62f554a9837071d8c4910dg , the Content Type is Entity , the ID for your instance of Entity is 6691462b32399974cfc2cb1a , and the template you want to render is our new TEI-XML-Person template. Given these assumptions, the URL would look like: https://mycorpora.org/corpus/62f554a9837071d8c4910dg/Entity/6691462b32399974cfc2cb1a/?render_template=TEI-XML-Person Your browser should then display the rendered output for your content as an XML document (screenshot from Google Chrome): The Corpus API for Python The corpus API for Python does most of the heavy lifting behind the scenes in terms of the C.R.U.D. (creating, reading, updating, and deleting) operations on corpus data. Understanding how to use it is crucial to using the corpus iPython notebook and writing plugins for Corpora. At its heart, the API leverages MongoEngine --an ORM for MongoDB designed to behave similarly to the SQL-oriented ORM baked into Django . In fact, each corpus or content object is a MongoEngine Document under the hood, and when you query for content using the corpus API, you're actually working with MongoEngine QuerySets . As such, the majority of the documentation for the corpus API is covered by MongoEngine's documentation, so much of the documentation here will be in the form of examples. Creating a Corpus from corpus import Corpus my_corpus = Corpus() my_corpus.name = \"MTC\" my_corpus.description = \"My Test Corpus\" my_corpus.save() Upon running the above script, the my_corpus variable will be an instance of mongoengine.Document . After saving it, the id property of my_corpus will be a BSON ObjectId, which is how MongoDB uniquely identifies each document. The alphanumeric string representation of the ObjectId can be acquired like so: my_corpus_id_as_a_string = str(my_corpus.id) Retrieving a Corpus from corpus import get_corpus my_corpus = get_corpus('6661e28c4399e45f0bfd2121') The get_corpus function will accept as its first parameter either a string or a BSON ObjectId and will return an instance of the Corpus object, which is ultimately a MongoEngine Document with the following properties: Property Data Type Purpose name string To provide a brief project label, typically an acronym. description string To provide a full project descriptor, typically the spelled out version of the project's name. uri string This property is generated when the corpus is first saved, and it provides a unique URI for the corpus as it exists on the instance of Corpora hosting it. path string Generated upon first save. It contains the file path to the corpus' files within the Corpora Docker container, should it have any. kvp dictionary KVP stands for \"key/value pairs,\" and it's intended to house arbitrary metadata (rarely used). files dictionary The keys for the dictionary are unique hashes based on the file's path. These are long alphanumeric strings. The values are File objects . repos dictionary The keys are the name given to the repo, and the values are Repo objects . open_access boolean A flag for determining whether a corpus is open access, making its read-only API publicly available. content_types dictionary The keys are the name of a given Content Type, and the values are a dictionary specifying the metadata for a given Content Type and its fields. provenance list A list of completed jobs for this corpus. A corpus object also has several methods which will be covered in subsequent sections. Creating Content With a corpus object in hand, you can create content using the corpus' get_content method. This example assumes you have a Content Type called \"Document\" in your corpus: new_document = my_corpus.get_content('Document') new_document.title = \"On Beauty\" new_document.author = \"Zadie Smith\" new_document.save() Calling your corpus' get_content method by only passing in the Content Type name will return an instance of a MongoEngine document with that Content Type's fields as properties to be set. Once you've set those field values, you call the save method to save the content and assign it a unique ID. Note: When saving content, the data is first saved to MongoDB. Post-save events then fire which also index the content in Elasticsearch and link the data in Neo4J. As such, saving content can be relatively time consuming, especially when saving in bulk. In cases where bulk saving needs to occur, you can turn off indexing and/or linking like so: # Saves to MongoDB and Neo4J, but not Elasticsearch: new_document.save(do_indexing=False) # Saves to MongoDB and Elasticsearch, but not Neo4J: new_document.save(do_linking=False) # Saves only to MongoDB: new_document.save(do_indexing=False, do_linking=False) If you later want to fire off a job to index and link all of your content, you can do so like this: my_corpus.queue_local_job(task_name=\"Adjust Content\", parameters={ 'content_type': 'Document', 'reindex': True, 'relabel': True }) Retrieving Content Your corpus' get_content method is also useful for retrieving content when you know either the ID or exact field values for your content. When using get_content to retrieve content, you're ultimately querying MongoDB: # Query for a single piece of content with the ID known: content = my_corpus.get_content('Document', '5f623f2a52023c009d73108e') print(content.title) \"On Beauty\" # Query for a single piece of content by field value: content = my_corpus.get_content('Document', {'title': \"On Beauty\"}, single_result=True) # Query for multiple pieces of content by field value: contents = my_corpus.get_content('Document', {'author': \"Zadie Smith\"}) for content in contents: print(content.title) \"White Teeth\" \"On Beauty\" # Query for all content with this Content Type: contents = my_corpus.get_content('Document', all=True) When retrieving a single piece of content, you receive a MongoEngine Document. When retrieving multiple pieces of content, you receive a MongoEngine QuerySet. QuerySets are generators (can be iterated over using a for-loop), but also have their own methods, like count : contents = my_corpus.get_content('Document', all=True) contents.count() 42 Editing Content Once you've retrieved a single piece of content using get_content , you can directly edit its field values and then call save to edit it: content = my_corpus.get_content('Document', '5f623f2a52023c009d73108e') content.published_year = 2005 content.save() Deleting Content Deleting content is as simple as calling the MongoEngine Document's delete method: content = my_corpus.get_content('Document', '5f623f2a52023c009d73108e') content.delete() Note: because content can be cross-referenced in arbitrary ways, deleting content saves a stub in the database that tells Corpora to sweep for instances of other content that references the deleted content so as to remove those references. When deleting large quantities of content, this can cause a backlog of deletion stubs. If you know you'll be deleting a large amount of content, and you also feel certain there's no need to track these deletions in order to hunt for stale content references, you can skip the creation of a deletion stub like so: content.delete(track_deletions=False) Working with Cross-Referenced Content Much of the value of Corpora's Neo4J database is in its ability to keep track of the way your content is related, allowing the interface to visualize these connections. Content becomes \"related\" to other content via fields of type cross-reference . By way of example, let's assume you're working with a corpus that has two Content Types: Entity and Letter . And let's say that the Letter has a field called recipient of type cross-reference that specifically references the type Entity . In this way, a Letter can reference a specific Entity via its recipient field. Let's create an Entity and a Letter, and \"relate\" them appropriately: entity = my_corpus.get_content('Entity') entity.name = \"Elizabeth Barrett Browning\" entity.save() letter = my_corpus.get_content('Letter') letter.contents = \"Real warm spring, dear Miss Barrett, and the birds know it, and in Spring I shall see you, really see you...\" letter.recipient = entity.id letter.save() Note how when specifying the value of the recipient field for our instance of Letter , we used the id property of Entity . If the \"multiple\" box is checked when creating a field of type cross-reference , the field is actually a list, and so content ID's must be appended to the list: letter.recipients.append(entity.id) letter.save() You may query for content using cross-referenced fields, and the easiest way to do this is with the ObjectId (or its string representation) of the cross-referenced content. For example: letters_to_elizabeth = my_corpus.get_content('Letter', {'recipient': '66a166e56cf2fb23103b58b2'}) You may also access the values of nested fields for cross-referenced content like so: first_letter = letters_to_elizabeth[0] print(first_letter.recipient.name) \"Elizabeth Barrett Browning\" Working with Files In Corpora, files belonging to a corpus or to a piece of content are ultimately registered as File objects, which themselves are a MongoEngine Embedded Document with the following properties: Property Data Type Purpose path string To keep track of the file path as it exists inside the Corpora container. primary_witness boolean To flag whether the file should be the primary \"witness\" or digital surrogate for the content in question. Currently only used in the context of the Document plugin. basename string The filename of the file (without the path), i.e. \"data.csv\" extension string The extension of the file, i.e. \"csv\" byte_size integer The size of the file in bytes description string Human-readable description of the file provenance_type string To track what kind of thing originated this file, i.e. \"Tesseract OCR Job\" provenance_id string A unique identifier for the thing that originated this file, i.e. \"4213\" height integer The height in pixels of the file (if it's an image) width integer The width in pixels of the file (if it's an image) iiif_info dict A dictionary representing the kind of metadata you get when querying for /info.json on a IIIF server While those properties can be helpful, the only required property when creating a File object to represent a file is the path . The path of a file is always relative to the Corpora container. When the file is directly associated with a corpus, it lives in the /files directory, itself living in the directory specified by the corpus' path property. A directory is created in the Corpora container any time a corpus is created, and its path always looks like /corpora/[corpus ID] . As such, files directly associated with a corpus should live inside the /corpus/[corpus ID]/files directory. The best way to associate a file with a corpus is via the corpus' homepage by going to the \"Metadata\" tab and clicking the orange \"Import\" button next to \"Corpus Files.\" When you import files like this, it saves them in a files directory living inside of the directory specified in the path property of the corpus. Imported files are also registered in the files dictionary of your corpus object. The keys for the files dictionary of a corpus are hashes based on the file's path--this is to provide a URL-friendly way of accessing them. This makes retrieving them programmatically in Python a little unintuitive, however. Let's say you upload a file called entities.csv to your corpus. To access that file with Python, you'll need to get its path like so: entities_csv_path = None for file_key in my_corpus.files.keys(): if 'entities.csv' in my_corpus.files[file_key].path: entities_csv_path = my_corpus.files[file_key].path Content Types can also specify \"File\" as a type of field, and in those cases, you may directly access the path property of the file (no intervening dictionary with file key hashes): photo = my_corpus.get_content('Photo', '66a166e56cf2fb23103b6h7') photo.original_file.path When files belong to an instance of content (rather than directly associated with a corpus), that piece of content gets its own path property and a directory is created for it (directories are normally not created for content--only when they have files associated with them). Content paths always look like this: /corpora/[corpus ID]/[Content Type]/[breakout directory]/[content ID] Given that millions of instances of a Content Type could exist for a corpus, Corpora implements a \"breakout directory\" to prevent any one directory from containing millions of subdirectories! Much like with a corpus, files associated with content live inside the /files subdirectory of a content instance's path, i.e.: /corpora/[corpus ID]/[Content Type]/[breakout directory]/[content ID]/files When programmatically associating a file to a corpus or piece of content, it's important for that file to live in the correct place, as this allows all the files belonging to a corpus to be exported and restored appropriately. To programmatically associate a file directly with a corpus, first upload it to the corpus' appropriate /files subdirectory and make note of its full path. Then: from corpus import get_corpus, File # store the path to the file in a variable my_file_path = '/corpora/6661e28c4399e45f0bfd2121/files/data.csv' # retrieve your corpus my_corpus = get_corpus('6661e28c4399e45f0bfd2121') # create an instance of the File object by using its \"process\" method # which takes at minimum the path to the file my_file = File.process(my_file_path) # generate a file key for storing the file in the corpus my_file_key = File.generate_key(my_file_path) my_corpus.files[my_file_key] = my_file my_corpus.save() Note the use of the process class method of File. That method takes a file path, checks to see if the file exists, gathers some minimal metadata about the file (like file size), and returns a File object. Because files directly associated with a corpus are stored using a file key, we generate one with the generate_key method of File. The Corpus Notebook Corpora makes available to admins and corpus Editors the ability to launch an iPython notebook associated with your corpus. This is especially useful for loading and transforming data, or for developing code that will eventually live as a Task inside of a plugin. Launching the Notebook To launch the corpus notebook, navigate to your corpus' homepage and click on the \"Admin\" tab. Click the orange \"Launch Notebook\" button to the right of the \"Running Jobs\" section. This will cause the page to reload, and a message should appear saying Notebook server successfully launched! Access your notebook here. Click on the \"here\" link to open your notebook in a new browser tab. Setting up the Corpus Python API Once you've opened your notebook, you'll see that the first cell has been created for you. In it is code that must be run in order for you to make use of the Corpus Python API. Once you run that cell, you'll have access to the variable my_corpus which is an instance of the Corpus object . Python Packages For a list of Python packages installed in your notebook environment, see the requirements.txt file used by the build process for the Corpora container. This list of packages has been kept relatively minimal in order to keep the size of the Corpora container manageable. That said, additional packages can be installed using the following methods. Installing at Runtime To install a given package (like, say, pandas ) at runtime, simply prefix a pip install command with an exclamation point in a notebook cell and execute it. For example: !pip install pandas This will prompt pip (the Package Installer for Python) to download and install the package to the \"user installation\" of Python. Specifically, this is found at /conf/plugin_modules/lib/python3.11/site-packages inside the Corpora container. As instructed in the deployment documentation , you should have various subdirectories inside a data directory on your host machine mounted inside of Corpora, and one of those subdirectories is conf , corresponding to the /conf path inside the Corpora container. As such, assuming the Corpora data directory you set up on your host machine is at /corpora/data , you'll find packages installed in this manner in /corpora/data/conf/plugin_modules/lib/python3.11/site-packages on your host machine. Because these files are living in a directory mounted from your host computer, any packages you install at runtime will persist until you delete or uninstall them. Installing via Plugin Corpora is built with a plugin architecture allowing you to extend its functionality. One way to ensure that certain Python packages are installed when the Corpora container first launches is to specify them in a requirements.txt file in the directory for your plugin. Note that for Corpora to look for that requirements.txt file, your plugin must be enabled as instructed here , and enabling a plugin (as well as installing packages specified via requirements.txt ) requires that you restart the Corpora container. Once Corpora has installed your packages in this way, they are persisted at the same /conf/plugin_modules/lib/python3.11/site-packages location and won't have to be installed during subsequent restarts. Limitations While your notebook is saved as you go and can be returned to over multiple sessions, at this time Corpora only supports a single notebook per corpus. Also, at this time only a single notebook can be running on a given instance of Corpora at any given time . As such, should you be working in your notebook, and should another user attempt to launch their own corpus notebook, your notebook will be shut down and they will have the active notebook session. Building Corpora Plugins Corpora's plugin architecture allows you to extend the functionality of Corpora by adding custom Content Types, asynchronous Tasks, and even new REST API endpoints or public facing web pages. In this way, Corpora's codebase can remain relatively generic while the data schema and functionality of a custom project can be contained in a separate and distributable codebase. Minimum Requirements For Corpora to recognize your plugin, you must choose a name for your plugin that adheres to the Python convention for package and module names , i.e. short, all lowercase, and usually one word or acronym, though underscores are okay if necessary (no other special characters or spaces). For the purposes of this documentation, we'll assume you're creating a plugin named survey . Having chosen your name, create a directory with that name, and inside it place an empty file named __init__.py : survey \u2502 __init__.py Technically, placing this directory in the correct place, enabling the plugin, and restarting Corpora according to these instructions is all that is required for your plugin to work. At this point, however, the survey plugin does nothing useful. See below for the various ways to build functionality for your plugin. Custom Content Types Often, the various tasks and functionality of your plugin require specialized Content Types for storing idiosyncratic data. The tesseract plugin, for instance, requires a Content Type called TesseractLanguageModel in order to store models for specific languages or fonts that can be trained using Corpora's interface. As for our survey plugin, let's create some Content Types to keep track of surveys, questions, sessions, and responses. The easiest way to go about this is to create a new corpus and use the Content Type Manager to craft your Content Types . Here are the various Content Types we'll create: Survey Field Label Data Type name Name Text (English) open Open? Boolean Question Field Label Data Type survey Survey Cross-reference (Survey) query Query Text (English) query_choices Query Choices Text (English), multiple order Order Number Session Field Label Data Type survey Survey Cross-reference (Survey) respondent_name Respondent Name Text (English) respondent_email Respondent Email Keyword date_taken Date Taken Date Answer Field Label Data Type session Session Cross-reference (Session) question Question Cross-reference (Question) response Response Text (English) Once these Content Types have been created in your Content Type Manager, the easiest way to package them with your plugin is to download the JSON representation of your schema by clicking the export button in the footer of the Content Type Manager, which looks like this: Clicking the schema export button will kick off a download of the JSON representation of your corpus as a file named schema.json . The next step is to edit that file with any text editor to convert the JSON to valid Python code. This can be done with three simple Find and Replace operations: Replace \"true\" with \"True\" Replace \"false\" with \"False\" Replace \"null\" with \"None\" Once you've done this, create a file called content.py and save it in your plugin directory. survey \u2502 __init__.py \u2502 content.py Edit content.py , and create a single variable named REGISTRY , setting it equal to the Content Type schema you exported and converted to Python code. Here's a snippet of what the first lines of that file would look like now: REGISTRY = [ { \"name\": \"Survey\", \"plural_name\": \"Surveys\", \"fields\": [ { \"name\": \"name\", \"label\": \"Name\", \"indexed\": False, \"unique\": False, \"multiple\": False, \"in_lists\": True, \"type\": \"text\", ... ... ] Assuming you've enabled this plugin and restarted Corpora, you should now be able to go into the Content Type Manager for any corpus and click on the schema import button, which looks like this: This will open a pop-up modal with a text editor field for copying and pasting JSON. Below that field is a \"Import plugin schema...\" dropdown that looks like this: This dropdown will allow you to select one or all of the Content Types associated with your plugin. Once you've made your choice, click the orange Import button. Your Content Types should now appear in the Content Type Manager. Be sure to click Save Changes to commit that schema to your corpus. If you would like to add custom functionality to your Content Types that can be used in the context of Python, you can inherit the Content class and write your own code to override or extend existing properties and methods. To do this, you must set the inherited_from_module and inherited_from_class keys in the schema for your Content Type (as saved in the REGISTRY variable). You must also indicate whether certain fields are inherited from your custom class by setting inherited to True in your data schema for those fields. To see an example of this, see the schema for Document and the custom classes below it in the code for the Document plugin that comes built-in to Corpora. Custom Tasks Corpora's plugin architecture allows you to create asynchronous tasks that can be launched programmatically, as well as from the Admin interface of a given corpus (given certain conditions). The following will expand on our survey plugin by adding a task as an example. Developers are also encouraged to view the source code for some existing tasks, such as for the document and tesseract plugins. Much like with custom Content Types for plugins, when creating custom tasks you'll want to create a file called tasks.py and place it at the root level of your plugin directory, i.e.: survey \u2502 __init__.py \u2502 content.py \u2502 tasks.py In tasks.py , you'll need to create a variable called REGISTRY and set it equal to a Python dictionary, where the keys are the unique names for your tasks and the values are nested dictionaries with the specifications for your task. Below is an example for a task that will generate a survey report. The various settings in this example with be expounded upon below. REGISTRY = { \"Generate Survey Report\": { \"version\": \"0\", \"jobsite_type\": \"HUEY\", \"content_type\": \"Corpus\", \"track_provenance\": True, \"create_report\": True, \"module\": \"plugins.survey.tasks\", \"functions\": ['generate_survey_report'], \"configuration\": { \"parameters\": { \"session\": { \"value\": \"\", \"label\": \"Survey Session\", \"note\": \"Select the survey taking session for which to generate a report.\", \"type\": \"xref\", \"content_type\": \"Session\" } } }, } } Task Settings version ( string ): When tasks are first registered by Corpora, the task name, task version, and task parameters are copied to the database for quick retrieval. As you iteratively develop your task, you may need to adjust the available parameters. To get Corpora to re-register your tasks's parameters, you'll want to bump up the version number (i.e. change the value 0 to 1 ). jobsite_type ( string ): At present, only the value HUEY is supported, as tasks are only run using the huey task queue built into Corpora. In the future, other jobsite types (such as SLURM) may be supported. A jobsite is intended to be something like \"the server or environment where Corpora tasks are run.\" At present, Corpora creates a single jobsite called Local when first initialized, and that Local jobsite is of type HUEY . Jobs run on this local jobsite are run in the same Corpora container that serves the web application, and the number of jobs (or job processes) that can be run concurrently corresponds to the CRP_HUEY_WORKERS environment variable for the Corpora container, which by default is 10 . content_type ( string ): All tasks must target (or be associated with) a particular Content Type. While a corpus is technically composed of Content Types, Corpus is a valid value here. Tasks that have Corpus as their Content Type and track_provenance set to True may be launched from the Admin tab of a corpus. track_provenance ( boolean ): Setting track_provenance to True causes Corpora to save a record of having run a job with this task on the target. From within the Corpus Python API , recorded jobs can be viewed by referencing the provenance property of a given instance of content, as well of a corpus itself. Setting track_provenance to False will prevent Corpora from saving a record of jobs run with this task, and will also prevent users from being able to launch the job from the Admin tab of the corpus. This is useful for tasks that will only be run programmatically. create_report ( boolean ): Setting create_report to True will cause any jobs running this task to create a plain text file with some generic information about the job at the top. It also allows for the task to write messages to this file about the task as it runs. For jobs that are launched from the Admin tab of a corpus, those reports can be read via the web interface for Corpora both while a job is running and after it has been completed. Examples for writing messages to the report can be viewed in the example code for a task provided further on in this example. module ( string ): This is the path to the module containing the function(s) that will be run for this task, and is almost always plugins.[plugin name].tasks . functions ( list of strings ): This is a list of the function names that should be called sequentially as part of the task. Each of these names must correspond to functions that live in the module specified by the module setting described above. Normally this will include only a single function, but some situations may require that multiple functions be run one after the other. Corpora's job queue is capable of determining when one function's execution has completed, and will pass the job onto the next function in this list until all have been completed. For an example of this kind of function execution chaining, see the OCR Document with Tesseract 5 task. configuration ( dictionary ): This is intended to store configuration options that are specific to the jobsite, including the available parameters for the user to provide when launching the job. At present, the only key that Corpora looks for is parameters , though in the future more keys may live here, such as settings specific to SLURM. configuration -> parameters ( dictionary ): This dictionary is nested under the parameters key which is part of the configuration dictionary. It's intended to provide the various parameters a user can configure when launching the job--when launching a job from the Admin tab in Corpora, these parameters are used to construct a web form for the user. The keys for this dictionary are intended to be the names of parameters passed into the job, and should follow the PEP 8 convention for variable names . The value for these keys should be a dictionary with settings that specify what type of parameter this is, what the default value should be, what the label for the parameter should be on the constructed web form, and the text for a note that is provided on the web form for that parameter. As such, type , value , label , and note are keys you can use for this dictionary that Corpora will recognize and act on accordingly. Depending on the value of type , certain other keys may also be expected. Below is a table describing the various parameter types. Type Example Value Web Form Equivalent Additional Parameter Keys Expected text \"OCR run 42\" Input of type text None boolean True Checkbox None password \"secretphrase\" Input of type password (text entered here is obscured by asterisks) None pep8_text \"like_a_variable_name\" Input of type text, with PEP 8 formatting enforced via JavaScript None choice \"Option B\" A dropdown menu with the options specified by the additional choices key choices , which is a list of strings xref The string representation of an ObjectId A UI for selecting a specific piece of content of the Content Type specified by the additional content_type key content_type , which is a string corpus_file \"/path/to/a/file\" A dropdown menu with filenames as options (these must be files directly associated with a corpus via the Metadata tab) None corpus_repo \"project-tei\" A dropdown menu with the names of Git repositories as options (these must be directly associated with a corpus via the Metadata tab) None content_type \"Survey\" A dropdown menu with corpus Content Types as options None content_type_field \"Survey->name\" A dropdown menu with the fields for corpus Content Types as options None When a user launches a job, their selection for a given parameter is stored in the value key for a given parameter and passed to the task's function(s) by way of a job object (described below). Task Functions When tasks are registered by Corpora at startup, Corpora stores the name of the function(s) to call when a job is launched with that task (a job is ultimately a task that is run at a particular place and time and by a particular user). Those functions are normally defined in tasks.py below the REGISTRY variable specified above. Here's an example of a simple task function for our survey plugin with details explained below: from huey.contrib.djhuey import db_task from corpus import Job REGISTRY = { ... } # The db_task decorator is necessary so Huey is aware of it as an available task to run. It also allows you to specify # the priority for this task, which only matters if more jobs are queued than there are Huey workers available to run # them. In those situations, the higher the integer you assign to priority, the more priority it has to run over tasks # with a lower priority. @db_task(priority=4) def generate_survey_report(job_id): # Task functions always take a single parameter, which is the ID of the job. job = Job(job_id) # You instantiate a Job object by just passing in its ID job.set_status('running') # The set_status method for job allows you to notify Corpora that the job is running # Getting the value of the 'session' parameter which was selected by the user: session_id = job.get_param_value('session') # Session is a custom Content Type specified in content.py for our plugin. We can get an instance of that Content # Type by calling the get_content method of our corpus object, which is always set for an instance of Job: session = job.corpus.get_content('Session', session_id) # Checking to see if we were able to successfully retrieve an instance of Session: if session: # Calling the 'report' method of the job object to log a message to the job report: job.report(f\"Survey report for {session.survey.name} taken by {session.respondent_name}:\") # The get_content method of the corpus object can take as a second parameter a dictionary specifying our # query, and returns a MongoEngine Queryset with the results. The QuerySet object has an order_by method # that takes as one or more parameters the fields you wish to sort by. In this case we're sorting by the # 'order' field (the + symbol as a prefix specifies that we're sorting in ascending order): questions = job.corpus.get_content('Question', {'survey': session.survey.id}).order_by('+order') # Here we're retrieving the number of questions for the survey so we can report a percentage complete # for our job as we iterate over them. In this example, the questions and answers will be iterated over # nearly instantaneously, but it's included here for reference purposes: total_questions = questions.count() questions_reported = 0 # Querysets can be iterated over, yielding MongoEngine documents corresponding to each piece of content: for question in questions: # Here we're retrieving the survey taker's answer to this question. When passing in True for the optional # 'single_result' keyword parameter, we're specifying that we want a single MongoEngine document as the # result (instead of a Queryset): answer = job.corpus.get_content('Answer', {'session': session_id, 'question': question.id}, single_result=True) # Here we're simply outputting to the job report the language of the question, and on the line directly # below it outputting the user's response: job.report(f\"QUESTION: {question.query} \\n ANSWER: {answer.response}\\n\\n\") # Here we're reporting our job progress in terms of percent complete using the set_status method of our job: questions_reported += 1 job.set_status('running', percent_complete=int( (questions_reported / total_questions) * 100 )) # Now that we've successfully completed our task, we use the 'complete' method of our job object, passing in the # status as the first parameter: job.complete('complete') # Since no session was found, we error out our job by marking it as complete and also passing in 'error' as the # status for our job. We're also providing an error message to report: else: job.complete('error', error_msg=\"No session was found for generating this report!\") The example task function above is intended to take a completed survey and generate a simple report by providing the survey questions with the user's response printed beneath them. Here are some key takeaways: Task functions should use the db_task decorator, and can optionally specify a priority (higher = more priority) for the function as it competes with other tasks on Corpora's job queue. Task functions take a single parameter, which is the ID for the job. Once Job objects are instantiated using the job ID, you can access the pertinent corpus for the job using the corpus property of the job object. The status of a running job can be set using the set_status method, including an optional percent_complete parameter to report on the amount of progress a job has made toward completion. Jobs are then completed using the complete method of Job, passing in either \"complete\" or \"error\" to indicate job completion or erroring out respectively. If a job is errored out, an optional error message can be specified using the error_msg keyword parameter. Subprocesses At times, it might make sense to spawn subprocesses in order to complete a complicated task in parallel. Subprocesses also make use of the Huey job queue to run, and from Huey's perspective, they are just another job. The Job object in Corpora, however, has affordances for considering these jobs as subprocesses of an already running job. Here's an example, assuming a Corpora task that takes as a parameter a path to a document to be OCR'd: from huey.contrib.djhuey import db_task from corpus import Job REGISTRY = { ... } @db_task(priority=4) def ocr_document(job_id): job = Job(job_id) document_path = job.get_param_value('document_path') job.set_status('running') # Assume code here for building a list of paths for page images extracted from the document, stored in a list called 'page_paths' page_paths = [ ... ] for page_path in page_paths: huey_task = ocr_page(job_id, page_path) job.add_process(huey_task.id) @db_task(priority=3, context=True) def ocr_page(job_id, page_path, task=None): job = Job(job_id) # Assume the existence of another function that actually performs the OCR given the path to a page actually_do_the_ocr(page_path) if task: job.complete_process(task.id) Here are some key takeaways from the example: Functions that are intended to be called and run as subprocesses should also have the db_task decorator, but must set the optional keyword parameter context to True . This ensures that when Huey runs the job by calling the task function, it passes in contextual information, namely a Task object with Huey's internal task ID. You can simply call functions that have the db_task decorator. When you do this, the function is queued up to be run in Huey's job queue, and a Huey Task object is immediately returned, which has id as a property. When you pass that ID into the add_process method of the Corpora Job object, Corpora registers that ID as a subprocess for the running job. Corpora will not consider the job complete until all subprocesses are marked as complete. To mark a subprocess as complete, simply call the complete_process method of Job, passing in Huey's internal task ID as its identifier. When using subprocesses like this, it's not necessary to manually mark the Corpora job as complete--Corpora's own job manager will do so once all subprocesses have been marked as completed. For an example of the use of subprocesses, see the tesseract plugin, and specifically the interaction between the ocr_document_with_tesseract and ocr_page_with_tesseract . Beyond Content and Tasks Because Corpora plugins are essentially Django apps, you can also create custom views and templates, include static files, and even create an entire frontend for your plugin. Documentation for this level of custom development is beyond the scope here, as it essentially involves Django development. You can see an example of a fully fledged frontend for a plugin by examining the codebase for the New Variorum Shakespeare plugin for Corpora . Developer FAQ's This section is intended to answer certain questions about how and why Corpora is built the way it is, and is intended for potential contributors to Corpora's codebase. Why Django? Django is a popular web framework written in Python. Python is one of the easiest procedural programming languages to learn, and is increasingly the lingua franca of the humanities and sciences, due to the strong support it has for things like machine learning and natural language processing. Why so many databases? One of the most glaring departures from the typical Django development pattern for Corpora is its reliance on a document (rather than relational) model for storing the majority of its data. There has long been interest in a document-based approach to writing Django apps, as evidenced by mongoengine , and MongoDB's own efforts to create an officially supported ORM for Django . This interest is driven by a desire for a more flexible, dynamic data schema that can change on-the-fly without so much hassle. This is one of Corpora's biggest conceits: it attempts to support project data schemas that are capable of evolving organically as the project develops, as is particularly appropriate to Digital Humanities projects. Consider, for instance, what needs to happen when a user decides they want their data schema to support a many-to-many relationship. In the relational paradigm, this involves the need for an entirely new table (a cross-table) to track those relationships and requires any queries for that data to usually involve at least three tables. Should performant queries for this data become an important requirement, supporting both the retrieval of this data and retaining the ability to sort and filter on fields other than the primary keys for that cross-table generally requires some retooling of indexes, etc. In the document-based paradigm, the establishment of a many-to-many relationship requires the addition of a single, multi-valued field to one of the two document collections. To borrow analogies from the object-oriented programming world, the relational model requires you to create an entirely new object (representing that cross-table), whereas the document-based model only requires adding a new property, i.e. an array, to one of the two objects in the relationship. The complexity introduced by things like cross-tables required by the relational paradigm echoes across much of what Corpora tries to support, such as the ability to quickly export project data in the form of JSON. With the document-based model, this is a direct export (MongoDB document collections are expressed naturally as JSON). A cross-table would require at the very least either a new JSON file specifically for the relationship between objects or some kind of logic that embeds that relationship in one or the other JSON representations of the related objects. The use of ElasticSearch and Neo4J are attempts to use databases that are tuned and purpose-built to perform full-text searching, sorting, and aggregating (ElasticSearch); and also to traverse the relational connections between datapoints in the form of a network graph (Neo4J). While this seems like both a redundancy in terms of data storage and the addition of a layer of complexity, the data living in those two databases are added/deleted/modified downstream (using signals) from any MongoDB operations, making them low-effort/high-reward additions. While for certain very large datasets that are constantly changing (on the order of hundreds of millions of datapoints and thousands of transactions per second), such duplication of data would have negative impacts both in terms of storage requirements and performance. The vast majority of Digital Humanities projects, however, are small in size and relatively static. For those projects, what matters most is flexibility in terms of the data schema, not efficiency in terms of storage or the performance of CRUD operations. It may seem ridiculous that despite making use of MongoDB, ElasticSearch, and Neo4J, Corpora also relies on an sqlite relational database . This is an unfortunate side-effect of the fact that certain low-hanging-fruit, out-of-the-box functionalities of Django (like user management and authentication) require the use of its baked-in ORM for relational databases. The aforementioned project to create an official MongoDB ORM for Django may eventually obviate the need for this database. Where does the core of Corpora's functionality live, code-wise? The heart of Corpora's backend codebase lives here . This is a giant file containing the classes that define a Corpus, the Content Types that describe the various kinds of objects comprising a given corpus, the Fields that describe the various properties for those objects, and the abstract Content class that a given instance of content inherits in order to present a mongoengine Document dynamically built according to a given Content Type. Inside that giant file are also classes that describe JobSites, Tasks, and Jobs in order to support Corpora's asynchronous job queue. The frontend \"heart\" of Corpora's codebase is broken up into object-based files found in this directory . These JavaScript objects provide functionality for much of Corpora's web interface, including its Content Tables, Graphs, and Groups; as well as its Job Manager. Why is this codebase missing certain necessities like comment headers above functions and code tests? When I acquired my computer science degree in the early 2000's, certain coding conventions had not yet been widely embraced by developer culture at large--most code, for instance, did not live in git repositories! One of the conventions that had yet to become normalized is the tendency to break large files into smaller ones, ideally at least one file per class. Another is to always create copious documentation inside one's code, with the bare minimum being headers above each function that can be used to automatically generate API-level code documentation. The final, and perhaps most important convention that had not yet become normalized was the creation of code-tests that can be run any time changes to a codebase are made. I realize that for Corpora to become more amenable to the developer culture at large, these housekeeping tasks are crucial. Particularly the creation of code-tests given the size and complexity of the project. My only excuse for the lack of these necessities so far is that the majority of Corpora's codebase was developed as a way to manage/develop multiple DH projects at once, each with its own idiosyncratic needs in terms of a data schema, while also each sharing the need for things like an interface for browsing and managing data, a REST API for accessing that data using 3rd-party tools or frontends, and a task queue for performing potentially long-running tasks to ingest or transform data. Rather than create 20 boutique web applications, I chose to focus my development time on a single core codebase that could grow over time to serve an increasingly diverse array of projects. I have also historically been the only full-time developer at the center I work for, so I just haven't had time to devote to making Corpora more friendly for other developers (and therefore more sustainable as a project). My hope is that time to tend to these matters will manifest at my current position, or that one of the projects relying on Corpora will be awarded grant funds to hire external entities to perform this work. I also realize much of the tedium of things like creating header comments above classes and functions could be alleviated by the use of AI, so long as I also have the time to vet the results. Why in the world would you use jQuery? In the modern JavaScript development world, the use of jQuery is a crutch at best. For the first half of my career, however, the vast majority of the development work I performed was on the backend. While I've always been a full-stack developer, I wrote C# code and relied on the ASP.NET framework to create most of my applications. That framework (at least at the time) relied much more intensely on things like visual interface builders and \"postbacks,\" such that it was rare that I needed to write more than a few lines of JavaScript. As such, in my initial forays into the language I very much looked for shortcuts and fell into the bad habit of using jQuery. I will slowly be untethering Corpora's frontend codebase from that library over time. Rather unintentionally, however, my reliance on jQuery and the fact that I never got on the transpiler bus for JavaScript has resulted in what I now consider to be a good thing: no iota of Corpora's JavaScript codebase requires transpilation, which means there are zero NPM-style dependencies. In the world of DH, funding comes and goes, which means that projects can go years without being touched. The JavaScript world (particularly the one that relies entirely on transpilation), however, moves very quickly. I've encountered several projects where the build dependencies for a project have gone stale to the point of requiring a major refactor after a mere year or two of sitting on the shelf. This unintentional place I find myself in has actually become a kind of philosophical stance when it comes to the Corpora codebase: at no point should its procedural code require a build step. While this may feel maddening to a frontend developer that relies on frameworks like React, I'm rather stubborn on this point, as it somewhat future-proofs its frontend codebase. Why is a JavaScript error thrown on individual Content pages in the browser's inspect panel? This error is a result of a hack I've made to the vis-network.js package that creates Corpora's network graphs. I've modified the physics algorithm to support what I call \"directional wind,\" and added a section to the config that gets passed in to instantiate each graph, one that specifies a direction that blows each type of node in a particular direction for each iteration of the physics engine. This wind allows for the graph to tease itself out and become far more legible by subtly pushing similar nodes in the same direction, regardless of where they fall on the graph. I have made a corresponding feature request to this effect here . Unfortunately, viz.js rigorously vets its config objects and throws an error any time something is amiss, hence the big ugly error in the JavaScript console.","title":"Developing"},{"location":"developing/#developing","text":"Corpora was built with the DH developer in mind just as much as the DH scholar. It was created as a way to flexibly handle a wide variety of DH projects with minimal effort while also empowering the developer to focus on the more innovative aspects of a given project. Below are detailed the affordances of corpora tailored specifically for developers, listed in order of complexity.","title":"Developing"},{"location":"developing/#content-type-templates","text":"From a developer's perspective, a Content Type in Corpora is a class whose properties and methods are largely defined by the Content Type Manager (essentially a data schema editor) available on the Admin tab of a given corpus. Content Type Templates are the convention Corpora uses to allow developers to control how instances of content get rendered , whether as a textual label, an HTML snippet, a JavaScript component, an XML document, etc. This rendering is done using the Jinja2-style Django template convention , and indeed it's recommended to refer to Django's documentation when editing or creating templates (particularly as a reference for Django's built-in template tags ).","title":"Content Type Templates"},{"location":"developing/#editing-label-templates","text":"By way of example, let's consider the following Content Type used to keep track of named entities throughout a corpus of XML documents: There are four fields defined for this Content Type, which means any instance of this content will be an object with at least four properties. If we were to name an instance of this class Entity , then you'd access its four properties like this using pseudocode: Entity.xml_id Entity.entity_type Entity.name Entity.uris There are also always three more hidden properties available for any instance of Content: Entity.id # a unique, alphanumeric identifier Entity.uri # a unique URI for this content which includes its corpus ID Entity.label # a textual representation of the content The label property for any given piece of content in Corpora is generated using the template called \"Label\" which can be edited using the Content Type Manager. To edit the Label template, go to the Admin tab of your corpus, scroll to the Content Type Manager, and expand out the tray for a given Content Type. Scroll to the bottom of that tray, and locate in the footer of the table that lists the fields for your Content Type the dropdown prefixed with the label \"Edit Template.\" Click the \"Go\" button to begin editing the template used by Corpora to generate textual labels for that Content Type: In this particular case, the template for our label looks like this: {{ Entity.name }} ({{ Entity.entity_type }}) When editing a Content Type template in Corpora, the template's \"namespace\" has available to it an instance of the content named the same as the Content Type (i.e. Entity ). Django's templating system has a convention whereby you can dynamically insert the value for an object's property by surrounding it with double curly-braces. So to output the value of your Entity's name property in a template, you'd use: {{ Entity.name }} Notice this is how our template example begins. The rest of the example includes a space, open parenthesis, the output of the value for the Entity's entity_type field, and then a closing parenthesis. Given this template, if our instance of the Entity Content Type had the value \"Maria Edgeworth\" for the field name , and \"PERSON\" for the field entity_type , the textual label for that piece of content would look like this: Maria Edgeworth (PERSON) Django's templating system is powerful, as it also provides affordances for boolean logic in the form of if/else statements. Let's say we want our label template to be a little more sophisticated by having it provide default values for fields that have no value. In this case, we'll leverage Django's built-in {% if ... %} syntax : {% if Entity.name %}{{ Entity.name }}{% else %}Unknown{% endif %} ({% if Entity.entity_type %}{{ Entity.entity_type }}{% else %}UNKNOWN{% endif %}) Using this new template, if the Entity's name property has no value, the string \"Unknown\" will be output. Similarly, if entity_type has no value, \"UNKNOWN\" will be output. Note that when you make changes to a template in Corpora's Content Type Manager, you must click the orange \"Save\" button on the \"Edit Template\" modal, and must also click the orange \"Save Changes\" button in the footer of the Content Type Manager for template changes to be \"committed\" to your data schema. Also note that when the Label template is changed, Corpora automatically fires off a reindexing task for the Content Type in question, as well as for any other Content Types in your corpus that reference the Content Type in question. Depending on how many instances of these Content Types you have in your corpus, this reindexing may take some time.","title":"Editing Label Templates"},{"location":"developing/#creating-new-templates","text":"Beyond specifying how content labels get created, Corpora's Content Type templating system allows you to create almost any kind of web-based representation of your content by allowing you to build a template and choose the appropriate MIME type for that representation. Building off our Entity example, let's say you wanted to create TEI XML representations for entities in your corpus. In the Content Type Manager for your corpus, you'd expand out the tray for your Content Type and, next to the \"Go\" button for editing an existing template, you'd click the \"New Template\" button to bring up the template editor: Give your template a URL-friendly name (no spaces or special characters), provide the content for your template, and choose an appropriate MIME Type (in this case, text/xml so we can serve up XML for the output of this template). In case the image above is too small or blurry, here's the content for this template: <person xml:id=\"{{ Entity.xml_id }}\"> <persName>{{ Entity.name }}</persName> </person> Click the \"Save\" button on the template editing modal, and then click \"Save Changes\" at the bottom of the Content Type Manager. Once this happens, your new template is available to be rendered.","title":"Creating New Templates"},{"location":"developing/#viewing-rendered-templates","text":"To view the output of a template for an instance of content, you'll need to construct a URL that follows this convention: [Your Corpora Instance]/corpus/[Corpus ID]/[Content Type]/[Content ID]/?render_template=[Template Name] In this example, let's assume your Corpora instance is hosted at https://mycorpora.org , your Corpus ID is 62f554a9837071d8c4910dg , the Content Type is Entity , the ID for your instance of Entity is 6691462b32399974cfc2cb1a , and the template you want to render is our new TEI-XML-Person template. Given these assumptions, the URL would look like: https://mycorpora.org/corpus/62f554a9837071d8c4910dg/Entity/6691462b32399974cfc2cb1a/?render_template=TEI-XML-Person Your browser should then display the rendered output for your content as an XML document (screenshot from Google Chrome):","title":"Viewing Rendered Templates"},{"location":"developing/#the-corpus-api-for-python","text":"The corpus API for Python does most of the heavy lifting behind the scenes in terms of the C.R.U.D. (creating, reading, updating, and deleting) operations on corpus data. Understanding how to use it is crucial to using the corpus iPython notebook and writing plugins for Corpora. At its heart, the API leverages MongoEngine --an ORM for MongoDB designed to behave similarly to the SQL-oriented ORM baked into Django . In fact, each corpus or content object is a MongoEngine Document under the hood, and when you query for content using the corpus API, you're actually working with MongoEngine QuerySets . As such, the majority of the documentation for the corpus API is covered by MongoEngine's documentation, so much of the documentation here will be in the form of examples.","title":"The Corpus API for Python"},{"location":"developing/#creating-a-corpus","text":"from corpus import Corpus my_corpus = Corpus() my_corpus.name = \"MTC\" my_corpus.description = \"My Test Corpus\" my_corpus.save() Upon running the above script, the my_corpus variable will be an instance of mongoengine.Document . After saving it, the id property of my_corpus will be a BSON ObjectId, which is how MongoDB uniquely identifies each document. The alphanumeric string representation of the ObjectId can be acquired like so: my_corpus_id_as_a_string = str(my_corpus.id)","title":"Creating a Corpus"},{"location":"developing/#retrieving-a-corpus","text":"from corpus import get_corpus my_corpus = get_corpus('6661e28c4399e45f0bfd2121') The get_corpus function will accept as its first parameter either a string or a BSON ObjectId and will return an instance of the Corpus object, which is ultimately a MongoEngine Document with the following properties: Property Data Type Purpose name string To provide a brief project label, typically an acronym. description string To provide a full project descriptor, typically the spelled out version of the project's name. uri string This property is generated when the corpus is first saved, and it provides a unique URI for the corpus as it exists on the instance of Corpora hosting it. path string Generated upon first save. It contains the file path to the corpus' files within the Corpora Docker container, should it have any. kvp dictionary KVP stands for \"key/value pairs,\" and it's intended to house arbitrary metadata (rarely used). files dictionary The keys for the dictionary are unique hashes based on the file's path. These are long alphanumeric strings. The values are File objects . repos dictionary The keys are the name given to the repo, and the values are Repo objects . open_access boolean A flag for determining whether a corpus is open access, making its read-only API publicly available. content_types dictionary The keys are the name of a given Content Type, and the values are a dictionary specifying the metadata for a given Content Type and its fields. provenance list A list of completed jobs for this corpus. A corpus object also has several methods which will be covered in subsequent sections.","title":"Retrieving a Corpus"},{"location":"developing/#creating-content","text":"With a corpus object in hand, you can create content using the corpus' get_content method. This example assumes you have a Content Type called \"Document\" in your corpus: new_document = my_corpus.get_content('Document') new_document.title = \"On Beauty\" new_document.author = \"Zadie Smith\" new_document.save() Calling your corpus' get_content method by only passing in the Content Type name will return an instance of a MongoEngine document with that Content Type's fields as properties to be set. Once you've set those field values, you call the save method to save the content and assign it a unique ID. Note: When saving content, the data is first saved to MongoDB. Post-save events then fire which also index the content in Elasticsearch and link the data in Neo4J. As such, saving content can be relatively time consuming, especially when saving in bulk. In cases where bulk saving needs to occur, you can turn off indexing and/or linking like so: # Saves to MongoDB and Neo4J, but not Elasticsearch: new_document.save(do_indexing=False) # Saves to MongoDB and Elasticsearch, but not Neo4J: new_document.save(do_linking=False) # Saves only to MongoDB: new_document.save(do_indexing=False, do_linking=False) If you later want to fire off a job to index and link all of your content, you can do so like this: my_corpus.queue_local_job(task_name=\"Adjust Content\", parameters={ 'content_type': 'Document', 'reindex': True, 'relabel': True })","title":"Creating Content"},{"location":"developing/#retrieving-content","text":"Your corpus' get_content method is also useful for retrieving content when you know either the ID or exact field values for your content. When using get_content to retrieve content, you're ultimately querying MongoDB: # Query for a single piece of content with the ID known: content = my_corpus.get_content('Document', '5f623f2a52023c009d73108e') print(content.title) \"On Beauty\" # Query for a single piece of content by field value: content = my_corpus.get_content('Document', {'title': \"On Beauty\"}, single_result=True) # Query for multiple pieces of content by field value: contents = my_corpus.get_content('Document', {'author': \"Zadie Smith\"}) for content in contents: print(content.title) \"White Teeth\" \"On Beauty\" # Query for all content with this Content Type: contents = my_corpus.get_content('Document', all=True) When retrieving a single piece of content, you receive a MongoEngine Document. When retrieving multiple pieces of content, you receive a MongoEngine QuerySet. QuerySets are generators (can be iterated over using a for-loop), but also have their own methods, like count : contents = my_corpus.get_content('Document', all=True) contents.count() 42","title":"Retrieving Content"},{"location":"developing/#editing-content","text":"Once you've retrieved a single piece of content using get_content , you can directly edit its field values and then call save to edit it: content = my_corpus.get_content('Document', '5f623f2a52023c009d73108e') content.published_year = 2005 content.save()","title":"Editing Content"},{"location":"developing/#deleting-content","text":"Deleting content is as simple as calling the MongoEngine Document's delete method: content = my_corpus.get_content('Document', '5f623f2a52023c009d73108e') content.delete() Note: because content can be cross-referenced in arbitrary ways, deleting content saves a stub in the database that tells Corpora to sweep for instances of other content that references the deleted content so as to remove those references. When deleting large quantities of content, this can cause a backlog of deletion stubs. If you know you'll be deleting a large amount of content, and you also feel certain there's no need to track these deletions in order to hunt for stale content references, you can skip the creation of a deletion stub like so: content.delete(track_deletions=False)","title":"Deleting Content"},{"location":"developing/#working-with-cross-referenced-content","text":"Much of the value of Corpora's Neo4J database is in its ability to keep track of the way your content is related, allowing the interface to visualize these connections. Content becomes \"related\" to other content via fields of type cross-reference . By way of example, let's assume you're working with a corpus that has two Content Types: Entity and Letter . And let's say that the Letter has a field called recipient of type cross-reference that specifically references the type Entity . In this way, a Letter can reference a specific Entity via its recipient field. Let's create an Entity and a Letter, and \"relate\" them appropriately: entity = my_corpus.get_content('Entity') entity.name = \"Elizabeth Barrett Browning\" entity.save() letter = my_corpus.get_content('Letter') letter.contents = \"Real warm spring, dear Miss Barrett, and the birds know it, and in Spring I shall see you, really see you...\" letter.recipient = entity.id letter.save() Note how when specifying the value of the recipient field for our instance of Letter , we used the id property of Entity . If the \"multiple\" box is checked when creating a field of type cross-reference , the field is actually a list, and so content ID's must be appended to the list: letter.recipients.append(entity.id) letter.save() You may query for content using cross-referenced fields, and the easiest way to do this is with the ObjectId (or its string representation) of the cross-referenced content. For example: letters_to_elizabeth = my_corpus.get_content('Letter', {'recipient': '66a166e56cf2fb23103b58b2'}) You may also access the values of nested fields for cross-referenced content like so: first_letter = letters_to_elizabeth[0] print(first_letter.recipient.name) \"Elizabeth Barrett Browning\"","title":"Working with Cross-Referenced Content"},{"location":"developing/#working-with-files","text":"In Corpora, files belonging to a corpus or to a piece of content are ultimately registered as File objects, which themselves are a MongoEngine Embedded Document with the following properties: Property Data Type Purpose path string To keep track of the file path as it exists inside the Corpora container. primary_witness boolean To flag whether the file should be the primary \"witness\" or digital surrogate for the content in question. Currently only used in the context of the Document plugin. basename string The filename of the file (without the path), i.e. \"data.csv\" extension string The extension of the file, i.e. \"csv\" byte_size integer The size of the file in bytes description string Human-readable description of the file provenance_type string To track what kind of thing originated this file, i.e. \"Tesseract OCR Job\" provenance_id string A unique identifier for the thing that originated this file, i.e. \"4213\" height integer The height in pixels of the file (if it's an image) width integer The width in pixels of the file (if it's an image) iiif_info dict A dictionary representing the kind of metadata you get when querying for /info.json on a IIIF server While those properties can be helpful, the only required property when creating a File object to represent a file is the path . The path of a file is always relative to the Corpora container. When the file is directly associated with a corpus, it lives in the /files directory, itself living in the directory specified by the corpus' path property. A directory is created in the Corpora container any time a corpus is created, and its path always looks like /corpora/[corpus ID] . As such, files directly associated with a corpus should live inside the /corpus/[corpus ID]/files directory. The best way to associate a file with a corpus is via the corpus' homepage by going to the \"Metadata\" tab and clicking the orange \"Import\" button next to \"Corpus Files.\" When you import files like this, it saves them in a files directory living inside of the directory specified in the path property of the corpus. Imported files are also registered in the files dictionary of your corpus object. The keys for the files dictionary of a corpus are hashes based on the file's path--this is to provide a URL-friendly way of accessing them. This makes retrieving them programmatically in Python a little unintuitive, however. Let's say you upload a file called entities.csv to your corpus. To access that file with Python, you'll need to get its path like so: entities_csv_path = None for file_key in my_corpus.files.keys(): if 'entities.csv' in my_corpus.files[file_key].path: entities_csv_path = my_corpus.files[file_key].path Content Types can also specify \"File\" as a type of field, and in those cases, you may directly access the path property of the file (no intervening dictionary with file key hashes): photo = my_corpus.get_content('Photo', '66a166e56cf2fb23103b6h7') photo.original_file.path When files belong to an instance of content (rather than directly associated with a corpus), that piece of content gets its own path property and a directory is created for it (directories are normally not created for content--only when they have files associated with them). Content paths always look like this: /corpora/[corpus ID]/[Content Type]/[breakout directory]/[content ID] Given that millions of instances of a Content Type could exist for a corpus, Corpora implements a \"breakout directory\" to prevent any one directory from containing millions of subdirectories! Much like with a corpus, files associated with content live inside the /files subdirectory of a content instance's path, i.e.: /corpora/[corpus ID]/[Content Type]/[breakout directory]/[content ID]/files When programmatically associating a file to a corpus or piece of content, it's important for that file to live in the correct place, as this allows all the files belonging to a corpus to be exported and restored appropriately. To programmatically associate a file directly with a corpus, first upload it to the corpus' appropriate /files subdirectory and make note of its full path. Then: from corpus import get_corpus, File # store the path to the file in a variable my_file_path = '/corpora/6661e28c4399e45f0bfd2121/files/data.csv' # retrieve your corpus my_corpus = get_corpus('6661e28c4399e45f0bfd2121') # create an instance of the File object by using its \"process\" method # which takes at minimum the path to the file my_file = File.process(my_file_path) # generate a file key for storing the file in the corpus my_file_key = File.generate_key(my_file_path) my_corpus.files[my_file_key] = my_file my_corpus.save() Note the use of the process class method of File. That method takes a file path, checks to see if the file exists, gathers some minimal metadata about the file (like file size), and returns a File object. Because files directly associated with a corpus are stored using a file key, we generate one with the generate_key method of File.","title":"Working with Files"},{"location":"developing/#the-corpus-notebook","text":"Corpora makes available to admins and corpus Editors the ability to launch an iPython notebook associated with your corpus. This is especially useful for loading and transforming data, or for developing code that will eventually live as a Task inside of a plugin.","title":"The Corpus Notebook"},{"location":"developing/#launching-the-notebook","text":"To launch the corpus notebook, navigate to your corpus' homepage and click on the \"Admin\" tab. Click the orange \"Launch Notebook\" button to the right of the \"Running Jobs\" section. This will cause the page to reload, and a message should appear saying Notebook server successfully launched! Access your notebook here. Click on the \"here\" link to open your notebook in a new browser tab.","title":"Launching the Notebook"},{"location":"developing/#setting-up-the-corpus-python-api","text":"Once you've opened your notebook, you'll see that the first cell has been created for you. In it is code that must be run in order for you to make use of the Corpus Python API. Once you run that cell, you'll have access to the variable my_corpus which is an instance of the Corpus object .","title":"Setting up the Corpus Python API"},{"location":"developing/#python-packages","text":"For a list of Python packages installed in your notebook environment, see the requirements.txt file used by the build process for the Corpora container. This list of packages has been kept relatively minimal in order to keep the size of the Corpora container manageable. That said, additional packages can be installed using the following methods.","title":"Python Packages"},{"location":"developing/#installing-at-runtime","text":"To install a given package (like, say, pandas ) at runtime, simply prefix a pip install command with an exclamation point in a notebook cell and execute it. For example: !pip install pandas This will prompt pip (the Package Installer for Python) to download and install the package to the \"user installation\" of Python. Specifically, this is found at /conf/plugin_modules/lib/python3.11/site-packages inside the Corpora container. As instructed in the deployment documentation , you should have various subdirectories inside a data directory on your host machine mounted inside of Corpora, and one of those subdirectories is conf , corresponding to the /conf path inside the Corpora container. As such, assuming the Corpora data directory you set up on your host machine is at /corpora/data , you'll find packages installed in this manner in /corpora/data/conf/plugin_modules/lib/python3.11/site-packages on your host machine. Because these files are living in a directory mounted from your host computer, any packages you install at runtime will persist until you delete or uninstall them.","title":"Installing at Runtime"},{"location":"developing/#installing-via-plugin","text":"Corpora is built with a plugin architecture allowing you to extend its functionality. One way to ensure that certain Python packages are installed when the Corpora container first launches is to specify them in a requirements.txt file in the directory for your plugin. Note that for Corpora to look for that requirements.txt file, your plugin must be enabled as instructed here , and enabling a plugin (as well as installing packages specified via requirements.txt ) requires that you restart the Corpora container. Once Corpora has installed your packages in this way, they are persisted at the same /conf/plugin_modules/lib/python3.11/site-packages location and won't have to be installed during subsequent restarts.","title":"Installing via Plugin"},{"location":"developing/#limitations","text":"While your notebook is saved as you go and can be returned to over multiple sessions, at this time Corpora only supports a single notebook per corpus. Also, at this time only a single notebook can be running on a given instance of Corpora at any given time . As such, should you be working in your notebook, and should another user attempt to launch their own corpus notebook, your notebook will be shut down and they will have the active notebook session.","title":"Limitations"},{"location":"developing/#building-corpora-plugins","text":"Corpora's plugin architecture allows you to extend the functionality of Corpora by adding custom Content Types, asynchronous Tasks, and even new REST API endpoints or public facing web pages. In this way, Corpora's codebase can remain relatively generic while the data schema and functionality of a custom project can be contained in a separate and distributable codebase.","title":"Building Corpora Plugins"},{"location":"developing/#minimum-requirements","text":"For Corpora to recognize your plugin, you must choose a name for your plugin that adheres to the Python convention for package and module names , i.e. short, all lowercase, and usually one word or acronym, though underscores are okay if necessary (no other special characters or spaces). For the purposes of this documentation, we'll assume you're creating a plugin named survey . Having chosen your name, create a directory with that name, and inside it place an empty file named __init__.py : survey \u2502 __init__.py Technically, placing this directory in the correct place, enabling the plugin, and restarting Corpora according to these instructions is all that is required for your plugin to work. At this point, however, the survey plugin does nothing useful. See below for the various ways to build functionality for your plugin.","title":"Minimum Requirements"},{"location":"developing/#custom-content-types","text":"Often, the various tasks and functionality of your plugin require specialized Content Types for storing idiosyncratic data. The tesseract plugin, for instance, requires a Content Type called TesseractLanguageModel in order to store models for specific languages or fonts that can be trained using Corpora's interface. As for our survey plugin, let's create some Content Types to keep track of surveys, questions, sessions, and responses. The easiest way to go about this is to create a new corpus and use the Content Type Manager to craft your Content Types . Here are the various Content Types we'll create: Survey Field Label Data Type name Name Text (English) open Open? Boolean Question Field Label Data Type survey Survey Cross-reference (Survey) query Query Text (English) query_choices Query Choices Text (English), multiple order Order Number Session Field Label Data Type survey Survey Cross-reference (Survey) respondent_name Respondent Name Text (English) respondent_email Respondent Email Keyword date_taken Date Taken Date Answer Field Label Data Type session Session Cross-reference (Session) question Question Cross-reference (Question) response Response Text (English) Once these Content Types have been created in your Content Type Manager, the easiest way to package them with your plugin is to download the JSON representation of your schema by clicking the export button in the footer of the Content Type Manager, which looks like this: Clicking the schema export button will kick off a download of the JSON representation of your corpus as a file named schema.json . The next step is to edit that file with any text editor to convert the JSON to valid Python code. This can be done with three simple Find and Replace operations: Replace \"true\" with \"True\" Replace \"false\" with \"False\" Replace \"null\" with \"None\" Once you've done this, create a file called content.py and save it in your plugin directory. survey \u2502 __init__.py \u2502 content.py Edit content.py , and create a single variable named REGISTRY , setting it equal to the Content Type schema you exported and converted to Python code. Here's a snippet of what the first lines of that file would look like now: REGISTRY = [ { \"name\": \"Survey\", \"plural_name\": \"Surveys\", \"fields\": [ { \"name\": \"name\", \"label\": \"Name\", \"indexed\": False, \"unique\": False, \"multiple\": False, \"in_lists\": True, \"type\": \"text\", ... ... ] Assuming you've enabled this plugin and restarted Corpora, you should now be able to go into the Content Type Manager for any corpus and click on the schema import button, which looks like this: This will open a pop-up modal with a text editor field for copying and pasting JSON. Below that field is a \"Import plugin schema...\" dropdown that looks like this: This dropdown will allow you to select one or all of the Content Types associated with your plugin. Once you've made your choice, click the orange Import button. Your Content Types should now appear in the Content Type Manager. Be sure to click Save Changes to commit that schema to your corpus. If you would like to add custom functionality to your Content Types that can be used in the context of Python, you can inherit the Content class and write your own code to override or extend existing properties and methods. To do this, you must set the inherited_from_module and inherited_from_class keys in the schema for your Content Type (as saved in the REGISTRY variable). You must also indicate whether certain fields are inherited from your custom class by setting inherited to True in your data schema for those fields. To see an example of this, see the schema for Document and the custom classes below it in the code for the Document plugin that comes built-in to Corpora.","title":"Custom Content Types"},{"location":"developing/#custom-tasks","text":"Corpora's plugin architecture allows you to create asynchronous tasks that can be launched programmatically, as well as from the Admin interface of a given corpus (given certain conditions). The following will expand on our survey plugin by adding a task as an example. Developers are also encouraged to view the source code for some existing tasks, such as for the document and tesseract plugins. Much like with custom Content Types for plugins, when creating custom tasks you'll want to create a file called tasks.py and place it at the root level of your plugin directory, i.e.: survey \u2502 __init__.py \u2502 content.py \u2502 tasks.py In tasks.py , you'll need to create a variable called REGISTRY and set it equal to a Python dictionary, where the keys are the unique names for your tasks and the values are nested dictionaries with the specifications for your task. Below is an example for a task that will generate a survey report. The various settings in this example with be expounded upon below. REGISTRY = { \"Generate Survey Report\": { \"version\": \"0\", \"jobsite_type\": \"HUEY\", \"content_type\": \"Corpus\", \"track_provenance\": True, \"create_report\": True, \"module\": \"plugins.survey.tasks\", \"functions\": ['generate_survey_report'], \"configuration\": { \"parameters\": { \"session\": { \"value\": \"\", \"label\": \"Survey Session\", \"note\": \"Select the survey taking session for which to generate a report.\", \"type\": \"xref\", \"content_type\": \"Session\" } } }, } }","title":"Custom Tasks"},{"location":"developing/#task-settings","text":"version ( string ): When tasks are first registered by Corpora, the task name, task version, and task parameters are copied to the database for quick retrieval. As you iteratively develop your task, you may need to adjust the available parameters. To get Corpora to re-register your tasks's parameters, you'll want to bump up the version number (i.e. change the value 0 to 1 ). jobsite_type ( string ): At present, only the value HUEY is supported, as tasks are only run using the huey task queue built into Corpora. In the future, other jobsite types (such as SLURM) may be supported. A jobsite is intended to be something like \"the server or environment where Corpora tasks are run.\" At present, Corpora creates a single jobsite called Local when first initialized, and that Local jobsite is of type HUEY . Jobs run on this local jobsite are run in the same Corpora container that serves the web application, and the number of jobs (or job processes) that can be run concurrently corresponds to the CRP_HUEY_WORKERS environment variable for the Corpora container, which by default is 10 . content_type ( string ): All tasks must target (or be associated with) a particular Content Type. While a corpus is technically composed of Content Types, Corpus is a valid value here. Tasks that have Corpus as their Content Type and track_provenance set to True may be launched from the Admin tab of a corpus. track_provenance ( boolean ): Setting track_provenance to True causes Corpora to save a record of having run a job with this task on the target. From within the Corpus Python API , recorded jobs can be viewed by referencing the provenance property of a given instance of content, as well of a corpus itself. Setting track_provenance to False will prevent Corpora from saving a record of jobs run with this task, and will also prevent users from being able to launch the job from the Admin tab of the corpus. This is useful for tasks that will only be run programmatically. create_report ( boolean ): Setting create_report to True will cause any jobs running this task to create a plain text file with some generic information about the job at the top. It also allows for the task to write messages to this file about the task as it runs. For jobs that are launched from the Admin tab of a corpus, those reports can be read via the web interface for Corpora both while a job is running and after it has been completed. Examples for writing messages to the report can be viewed in the example code for a task provided further on in this example. module ( string ): This is the path to the module containing the function(s) that will be run for this task, and is almost always plugins.[plugin name].tasks . functions ( list of strings ): This is a list of the function names that should be called sequentially as part of the task. Each of these names must correspond to functions that live in the module specified by the module setting described above. Normally this will include only a single function, but some situations may require that multiple functions be run one after the other. Corpora's job queue is capable of determining when one function's execution has completed, and will pass the job onto the next function in this list until all have been completed. For an example of this kind of function execution chaining, see the OCR Document with Tesseract 5 task. configuration ( dictionary ): This is intended to store configuration options that are specific to the jobsite, including the available parameters for the user to provide when launching the job. At present, the only key that Corpora looks for is parameters , though in the future more keys may live here, such as settings specific to SLURM. configuration -> parameters ( dictionary ): This dictionary is nested under the parameters key which is part of the configuration dictionary. It's intended to provide the various parameters a user can configure when launching the job--when launching a job from the Admin tab in Corpora, these parameters are used to construct a web form for the user. The keys for this dictionary are intended to be the names of parameters passed into the job, and should follow the PEP 8 convention for variable names . The value for these keys should be a dictionary with settings that specify what type of parameter this is, what the default value should be, what the label for the parameter should be on the constructed web form, and the text for a note that is provided on the web form for that parameter. As such, type , value , label , and note are keys you can use for this dictionary that Corpora will recognize and act on accordingly. Depending on the value of type , certain other keys may also be expected. Below is a table describing the various parameter types. Type Example Value Web Form Equivalent Additional Parameter Keys Expected text \"OCR run 42\" Input of type text None boolean True Checkbox None password \"secretphrase\" Input of type password (text entered here is obscured by asterisks) None pep8_text \"like_a_variable_name\" Input of type text, with PEP 8 formatting enforced via JavaScript None choice \"Option B\" A dropdown menu with the options specified by the additional choices key choices , which is a list of strings xref The string representation of an ObjectId A UI for selecting a specific piece of content of the Content Type specified by the additional content_type key content_type , which is a string corpus_file \"/path/to/a/file\" A dropdown menu with filenames as options (these must be files directly associated with a corpus via the Metadata tab) None corpus_repo \"project-tei\" A dropdown menu with the names of Git repositories as options (these must be directly associated with a corpus via the Metadata tab) None content_type \"Survey\" A dropdown menu with corpus Content Types as options None content_type_field \"Survey->name\" A dropdown menu with the fields for corpus Content Types as options None When a user launches a job, their selection for a given parameter is stored in the value key for a given parameter and passed to the task's function(s) by way of a job object (described below).","title":"Task Settings"},{"location":"developing/#task-functions","text":"When tasks are registered by Corpora at startup, Corpora stores the name of the function(s) to call when a job is launched with that task (a job is ultimately a task that is run at a particular place and time and by a particular user). Those functions are normally defined in tasks.py below the REGISTRY variable specified above. Here's an example of a simple task function for our survey plugin with details explained below: from huey.contrib.djhuey import db_task from corpus import Job REGISTRY = { ... } # The db_task decorator is necessary so Huey is aware of it as an available task to run. It also allows you to specify # the priority for this task, which only matters if more jobs are queued than there are Huey workers available to run # them. In those situations, the higher the integer you assign to priority, the more priority it has to run over tasks # with a lower priority. @db_task(priority=4) def generate_survey_report(job_id): # Task functions always take a single parameter, which is the ID of the job. job = Job(job_id) # You instantiate a Job object by just passing in its ID job.set_status('running') # The set_status method for job allows you to notify Corpora that the job is running # Getting the value of the 'session' parameter which was selected by the user: session_id = job.get_param_value('session') # Session is a custom Content Type specified in content.py for our plugin. We can get an instance of that Content # Type by calling the get_content method of our corpus object, which is always set for an instance of Job: session = job.corpus.get_content('Session', session_id) # Checking to see if we were able to successfully retrieve an instance of Session: if session: # Calling the 'report' method of the job object to log a message to the job report: job.report(f\"Survey report for {session.survey.name} taken by {session.respondent_name}:\") # The get_content method of the corpus object can take as a second parameter a dictionary specifying our # query, and returns a MongoEngine Queryset with the results. The QuerySet object has an order_by method # that takes as one or more parameters the fields you wish to sort by. In this case we're sorting by the # 'order' field (the + symbol as a prefix specifies that we're sorting in ascending order): questions = job.corpus.get_content('Question', {'survey': session.survey.id}).order_by('+order') # Here we're retrieving the number of questions for the survey so we can report a percentage complete # for our job as we iterate over them. In this example, the questions and answers will be iterated over # nearly instantaneously, but it's included here for reference purposes: total_questions = questions.count() questions_reported = 0 # Querysets can be iterated over, yielding MongoEngine documents corresponding to each piece of content: for question in questions: # Here we're retrieving the survey taker's answer to this question. When passing in True for the optional # 'single_result' keyword parameter, we're specifying that we want a single MongoEngine document as the # result (instead of a Queryset): answer = job.corpus.get_content('Answer', {'session': session_id, 'question': question.id}, single_result=True) # Here we're simply outputting to the job report the language of the question, and on the line directly # below it outputting the user's response: job.report(f\"QUESTION: {question.query} \\n ANSWER: {answer.response}\\n\\n\") # Here we're reporting our job progress in terms of percent complete using the set_status method of our job: questions_reported += 1 job.set_status('running', percent_complete=int( (questions_reported / total_questions) * 100 )) # Now that we've successfully completed our task, we use the 'complete' method of our job object, passing in the # status as the first parameter: job.complete('complete') # Since no session was found, we error out our job by marking it as complete and also passing in 'error' as the # status for our job. We're also providing an error message to report: else: job.complete('error', error_msg=\"No session was found for generating this report!\") The example task function above is intended to take a completed survey and generate a simple report by providing the survey questions with the user's response printed beneath them. Here are some key takeaways: Task functions should use the db_task decorator, and can optionally specify a priority (higher = more priority) for the function as it competes with other tasks on Corpora's job queue. Task functions take a single parameter, which is the ID for the job. Once Job objects are instantiated using the job ID, you can access the pertinent corpus for the job using the corpus property of the job object. The status of a running job can be set using the set_status method, including an optional percent_complete parameter to report on the amount of progress a job has made toward completion. Jobs are then completed using the complete method of Job, passing in either \"complete\" or \"error\" to indicate job completion or erroring out respectively. If a job is errored out, an optional error message can be specified using the error_msg keyword parameter.","title":"Task Functions"},{"location":"developing/#subprocesses","text":"At times, it might make sense to spawn subprocesses in order to complete a complicated task in parallel. Subprocesses also make use of the Huey job queue to run, and from Huey's perspective, they are just another job. The Job object in Corpora, however, has affordances for considering these jobs as subprocesses of an already running job. Here's an example, assuming a Corpora task that takes as a parameter a path to a document to be OCR'd: from huey.contrib.djhuey import db_task from corpus import Job REGISTRY = { ... } @db_task(priority=4) def ocr_document(job_id): job = Job(job_id) document_path = job.get_param_value('document_path') job.set_status('running') # Assume code here for building a list of paths for page images extracted from the document, stored in a list called 'page_paths' page_paths = [ ... ] for page_path in page_paths: huey_task = ocr_page(job_id, page_path) job.add_process(huey_task.id) @db_task(priority=3, context=True) def ocr_page(job_id, page_path, task=None): job = Job(job_id) # Assume the existence of another function that actually performs the OCR given the path to a page actually_do_the_ocr(page_path) if task: job.complete_process(task.id) Here are some key takeaways from the example: Functions that are intended to be called and run as subprocesses should also have the db_task decorator, but must set the optional keyword parameter context to True . This ensures that when Huey runs the job by calling the task function, it passes in contextual information, namely a Task object with Huey's internal task ID. You can simply call functions that have the db_task decorator. When you do this, the function is queued up to be run in Huey's job queue, and a Huey Task object is immediately returned, which has id as a property. When you pass that ID into the add_process method of the Corpora Job object, Corpora registers that ID as a subprocess for the running job. Corpora will not consider the job complete until all subprocesses are marked as complete. To mark a subprocess as complete, simply call the complete_process method of Job, passing in Huey's internal task ID as its identifier. When using subprocesses like this, it's not necessary to manually mark the Corpora job as complete--Corpora's own job manager will do so once all subprocesses have been marked as completed. For an example of the use of subprocesses, see the tesseract plugin, and specifically the interaction between the ocr_document_with_tesseract and ocr_page_with_tesseract .","title":"Subprocesses"},{"location":"developing/#beyond-content-and-tasks","text":"Because Corpora plugins are essentially Django apps, you can also create custom views and templates, include static files, and even create an entire frontend for your plugin. Documentation for this level of custom development is beyond the scope here, as it essentially involves Django development. You can see an example of a fully fledged frontend for a plugin by examining the codebase for the New Variorum Shakespeare plugin for Corpora .","title":"Beyond Content and Tasks"},{"location":"developing/#developer-faqs","text":"This section is intended to answer certain questions about how and why Corpora is built the way it is, and is intended for potential contributors to Corpora's codebase.","title":"Developer FAQ's"},{"location":"developing/#why-django","text":"Django is a popular web framework written in Python. Python is one of the easiest procedural programming languages to learn, and is increasingly the lingua franca of the humanities and sciences, due to the strong support it has for things like machine learning and natural language processing.","title":"Why Django?"},{"location":"developing/#why-so-many-databases","text":"One of the most glaring departures from the typical Django development pattern for Corpora is its reliance on a document (rather than relational) model for storing the majority of its data. There has long been interest in a document-based approach to writing Django apps, as evidenced by mongoengine , and MongoDB's own efforts to create an officially supported ORM for Django . This interest is driven by a desire for a more flexible, dynamic data schema that can change on-the-fly without so much hassle. This is one of Corpora's biggest conceits: it attempts to support project data schemas that are capable of evolving organically as the project develops, as is particularly appropriate to Digital Humanities projects. Consider, for instance, what needs to happen when a user decides they want their data schema to support a many-to-many relationship. In the relational paradigm, this involves the need for an entirely new table (a cross-table) to track those relationships and requires any queries for that data to usually involve at least three tables. Should performant queries for this data become an important requirement, supporting both the retrieval of this data and retaining the ability to sort and filter on fields other than the primary keys for that cross-table generally requires some retooling of indexes, etc. In the document-based paradigm, the establishment of a many-to-many relationship requires the addition of a single, multi-valued field to one of the two document collections. To borrow analogies from the object-oriented programming world, the relational model requires you to create an entirely new object (representing that cross-table), whereas the document-based model only requires adding a new property, i.e. an array, to one of the two objects in the relationship. The complexity introduced by things like cross-tables required by the relational paradigm echoes across much of what Corpora tries to support, such as the ability to quickly export project data in the form of JSON. With the document-based model, this is a direct export (MongoDB document collections are expressed naturally as JSON). A cross-table would require at the very least either a new JSON file specifically for the relationship between objects or some kind of logic that embeds that relationship in one or the other JSON representations of the related objects. The use of ElasticSearch and Neo4J are attempts to use databases that are tuned and purpose-built to perform full-text searching, sorting, and aggregating (ElasticSearch); and also to traverse the relational connections between datapoints in the form of a network graph (Neo4J). While this seems like both a redundancy in terms of data storage and the addition of a layer of complexity, the data living in those two databases are added/deleted/modified downstream (using signals) from any MongoDB operations, making them low-effort/high-reward additions. While for certain very large datasets that are constantly changing (on the order of hundreds of millions of datapoints and thousands of transactions per second), such duplication of data would have negative impacts both in terms of storage requirements and performance. The vast majority of Digital Humanities projects, however, are small in size and relatively static. For those projects, what matters most is flexibility in terms of the data schema, not efficiency in terms of storage or the performance of CRUD operations. It may seem ridiculous that despite making use of MongoDB, ElasticSearch, and Neo4J, Corpora also relies on an sqlite relational database . This is an unfortunate side-effect of the fact that certain low-hanging-fruit, out-of-the-box functionalities of Django (like user management and authentication) require the use of its baked-in ORM for relational databases. The aforementioned project to create an official MongoDB ORM for Django may eventually obviate the need for this database.","title":"Why so many databases?"},{"location":"developing/#where-does-the-core-of-corporas-functionality-live-code-wise","text":"The heart of Corpora's backend codebase lives here . This is a giant file containing the classes that define a Corpus, the Content Types that describe the various kinds of objects comprising a given corpus, the Fields that describe the various properties for those objects, and the abstract Content class that a given instance of content inherits in order to present a mongoengine Document dynamically built according to a given Content Type. Inside that giant file are also classes that describe JobSites, Tasks, and Jobs in order to support Corpora's asynchronous job queue. The frontend \"heart\" of Corpora's codebase is broken up into object-based files found in this directory . These JavaScript objects provide functionality for much of Corpora's web interface, including its Content Tables, Graphs, and Groups; as well as its Job Manager.","title":"Where does the core of Corpora's functionality live, code-wise?"},{"location":"developing/#why-is-this-codebase-missing-certain-necessities-like-comment-headers-above-functions-and-code-tests","text":"When I acquired my computer science degree in the early 2000's, certain coding conventions had not yet been widely embraced by developer culture at large--most code, for instance, did not live in git repositories! One of the conventions that had yet to become normalized is the tendency to break large files into smaller ones, ideally at least one file per class. Another is to always create copious documentation inside one's code, with the bare minimum being headers above each function that can be used to automatically generate API-level code documentation. The final, and perhaps most important convention that had not yet become normalized was the creation of code-tests that can be run any time changes to a codebase are made. I realize that for Corpora to become more amenable to the developer culture at large, these housekeeping tasks are crucial. Particularly the creation of code-tests given the size and complexity of the project. My only excuse for the lack of these necessities so far is that the majority of Corpora's codebase was developed as a way to manage/develop multiple DH projects at once, each with its own idiosyncratic needs in terms of a data schema, while also each sharing the need for things like an interface for browsing and managing data, a REST API for accessing that data using 3rd-party tools or frontends, and a task queue for performing potentially long-running tasks to ingest or transform data. Rather than create 20 boutique web applications, I chose to focus my development time on a single core codebase that could grow over time to serve an increasingly diverse array of projects. I have also historically been the only full-time developer at the center I work for, so I just haven't had time to devote to making Corpora more friendly for other developers (and therefore more sustainable as a project). My hope is that time to tend to these matters will manifest at my current position, or that one of the projects relying on Corpora will be awarded grant funds to hire external entities to perform this work. I also realize much of the tedium of things like creating header comments above classes and functions could be alleviated by the use of AI, so long as I also have the time to vet the results.","title":"Why is this codebase missing certain necessities like comment headers above functions and code tests?"},{"location":"developing/#why-in-the-world-would-you-use-jquery","text":"In the modern JavaScript development world, the use of jQuery is a crutch at best. For the first half of my career, however, the vast majority of the development work I performed was on the backend. While I've always been a full-stack developer, I wrote C# code and relied on the ASP.NET framework to create most of my applications. That framework (at least at the time) relied much more intensely on things like visual interface builders and \"postbacks,\" such that it was rare that I needed to write more than a few lines of JavaScript. As such, in my initial forays into the language I very much looked for shortcuts and fell into the bad habit of using jQuery. I will slowly be untethering Corpora's frontend codebase from that library over time. Rather unintentionally, however, my reliance on jQuery and the fact that I never got on the transpiler bus for JavaScript has resulted in what I now consider to be a good thing: no iota of Corpora's JavaScript codebase requires transpilation, which means there are zero NPM-style dependencies. In the world of DH, funding comes and goes, which means that projects can go years without being touched. The JavaScript world (particularly the one that relies entirely on transpilation), however, moves very quickly. I've encountered several projects where the build dependencies for a project have gone stale to the point of requiring a major refactor after a mere year or two of sitting on the shelf. This unintentional place I find myself in has actually become a kind of philosophical stance when it comes to the Corpora codebase: at no point should its procedural code require a build step. While this may feel maddening to a frontend developer that relies on frameworks like React, I'm rather stubborn on this point, as it somewhat future-proofs its frontend codebase.","title":"Why in the world would you use jQuery?"},{"location":"developing/#why-is-a-javascript-error-thrown-on-individual-content-pages-in-the-browsers-inspect-panel","text":"This error is a result of a hack I've made to the vis-network.js package that creates Corpora's network graphs. I've modified the physics algorithm to support what I call \"directional wind,\" and added a section to the config that gets passed in to instantiate each graph, one that specifies a direction that blows each type of node in a particular direction for each iteration of the physics engine. This wind allows for the graph to tease itself out and become far more legible by subtly pushing similar nodes in the same direction, regardless of where they fall on the graph. I have made a corresponding feature request to this effect here . Unfortunately, viz.js rigorously vets its config objects and throws an error any time something is amiss, hence the big ugly error in the JavaScript console.","title":"Why is a JavaScript error thrown on individual Content pages in the browser's inspect panel?"},{"location":"managing/","text":"Managing There are a few tasks, such as managing users, backing up and restoring corpus data, and installing plugins that Corpora admins may need to perform. Managing Users To manage the users (or Scholars) on your instance of Corpora, click the blue \"Manage Scholars\" button in the footer of the table listing corpora on the homepage of the web application. Once on the \"Scholars\" page, you should see a table listing all the users in your instance of Corpora. You may click on the column names to sort, you can use the search box, or page through this list in order to find your user. Once you've found them, click on their username which should also be an orange link. This will bring up the \"Manage Scholar\" modal. Granting/Revoking Admin Privileges Admins in Corpora are able to create a new corpus, view and edit all data for existing corpora, run all corpus notebooks, and run all tasks. You can make a user an admin for your instance of Corpora by clicking the orange \"Grant Admin Privileges\" button. Once a user is an admin, that same button will read \"Revoke Admin Privileges.\" Granting/Revoking Corpus Permissions In order for a user to edit content, run a corpus notebook, or run certain tasks, they must either be an Admin for the instance of Corpora or be granted \"Editor\" privileges on a specific corpus. To do the latter, expand the \"Corpus permissions\" tray. In the \"New Permission\" form, provide the name of the corpus, choose \"Editor\" from the dropdown menu, and click \"Set Permission.\" To revoke that privilege, find it under \"Existing Permissions\" and choose \"None\" from the dropdown. Granting/Revoking Job Permissions While admins can run all jobs, users with Editor permissions on a corpus must be explicitly granted permission to run specific jobs. To do so, expand out the user's \"Job permissions\" tray. You can either check the box next to the specific task they need to be able to run, or check the \"Local (HUEY)\" box, which grants them privileges to run every local task. Once you've checked the appropriate box(es), click the orange \"Set Permissions\" button. Job permissions can be revoked by unchecking boxes and clicking \"Set Permissions\" again. Changing Passwords When logged in, individual users can change their passwords by clicking on the orange \"My Account\" button on the top right of a page in Corpora. Should they need their passwords reset, however, you may do so by pulling up their \"Manage Scholar\" modal and expanding the \"Change password\" tray. You may then provide their new password, confirm it, and click the \"Change Password\" button. Backing Up and Restoring Corpus Data When logged in as user with admin privileges, you should see several buttons on the footer of the table that lists all of the corpora on the home page of your instance of Corpora. To back up or restore corpus data, click the blue \"Corpus Backups\" button to manage corpus backups. Backing Up Corpus data can be backed up as a \"gzipped tar file,\" which includes metadata about your corpus, any files uploaded to your corpus or content types, and MongoDB database dumps of all the content in your corpus. To create an backup file, select the relevant corpus from the \"Corpus\" dropdown, provide a name (a default name is optionally provided for you), and click the orange \"Create\" button. Depending on how large your corpus is, this can take some time. Once an backup has been created, however, it should show up under \"Existing Backups\" beneath the creation form upon refreshing the page. Multiple backups for the same corpus may be created--they must, however, have unique names. In order to restore a corpus or migrate it to another instance of Corpora, you may download the file by clicking on its orange link in the \"Backup File\" column of the Existing Backups table. It is strongly recommended that you do not rename this file after you download it. Restoring To restore a corpus backup, it must be listed in the \"Existing Backups\" table. If it's not listed there, scroll to the bottom of the page where it says \"Import a Backup File.\" You may simply drag and drop your downloaded backup file into the gray box to make it available.* Once your backup file appears in the \"Existing Backups\" table, you may click the blue \"Restore\" button to the right of it to commence the restore process. Note that the restore process maintains the original unique ID assigned to your corpus at the time of creation. As such, if you already have a version of that corpus on your instance of Corpora, you must first delete it before performing a restore. To delete an existing corpus you must be logged in as an admin user. You can then go to that corpus' home page, click on the \"Admin\" tab, scroll to the bottom, and use the corpus deletion form. Depending on the amount of data in your corpus, restoring can take a long time, as data is being restored to MongoDB, Elasticsearch, and Neo4J. * Note: Certain very large corpus backup files (usually over 3GB in size) may cause the upload process to timeout, preventing you from registering it as an available corpus to restore. If this happens, you will have to register it manually. To do so, navigate to the data directory of your instance of Corpora as it exists on the machine/server hosting your instance, i.e. /corpora/data . Inside that directory will be several subdirectories such as \"archive,\" \"link,\" \"search,\" etc. Go inside the subdirectory named \"corpora\" and find the directory named backups, i.e. /corpora/data/corpora/backups . Copy your large export file into this directory. You'll then have to execute a Django management command on the machine hosting Corpora by performing the following steps: Open a terminal or command prompt. Determine the name of the container running Corpora. You can do that by running the command docker ps . This will output information about all the containers running on your machine. In the \"NAMES\" column of this output, you'll want to find the container name for Corpora, which will either be something like corpora-corpora-1 if you're using Docker Compose, or corpora_corpora.1.[alphanumeric id] if you're using Docker Swarm. Make note of this container name. Execute the following command in your terminal: docker exec -it [container name] python3 manage.py register_backup_file [backup filename] Note that when providing the backup filename in the command above, you only need the name of the file (not the full path). Once that command executes and you get a message saying \"Backup file successfully registered,\" you should be able to see it listed as an available corpus backup to restore on the \"Corpus Backups\" page. Installing Plugins Corpora is designed with a plugin architecture that leverages Django's \"app\" convention. As such, installing plugins for Corpora is relatively easy, though you must have access to the filesystem of the server. A given plugin is ultimately a directory with a particular file structure (see creating plugins for Corpora ). To install a plugin, place its directory inside the \"plugins\" subdirectory living in the data directory for Corpora, i.e. /corpora/data/plugins . Once the plugin directory is copied there, you must also enable the plugin by adding it to the comma delimited list of enabled plugins stored in the CRP_INSTALLED_PLUGINS environment variable set for the Corpora container. The easiest way to do this is to edit the docker-compose.yml file, find the \"environment\" section of the \"corpora\" service, make sure the line specifying CRP_INSTALLED_PLUGINS isn't commented out, and add the name of the plugin directory to that variables' comma delimited value. Corpora must be restarted for it to be registered properly. And in order for containers to be aware of updated environment variables, the container must be stopped altogether and re-created by the Docker engine. To completely stop and restart Corpora while running with Docker Compose, issue these commands: # first change to your codebase directory with the docker-compose.yml file docker-compose down # wait 20 seconds or so for the containers to stop docker-compose up -d To completely stop and restart Corpora while running as a Swarm service, issue these commands: # first change to your codebase directory with the docker-compose.yml file docker stack rm corpora # wait 20 seconds or so for the containers to stop docker stack deploy corpora -c docker-compose.yml Scaling Corpora Should your instance of Corpora receive high traffic volume, it is architected in such a way as to support multiple instances of the Corpora container--this is due to the fact that they all rely on the Redis container for session management. Scaling in this way has only been tested with a Docker Swarm deployment , and may be accomplished by setting the scale key in the service configuration for Corpora in docker-compose.yml , or by issuing the appropriate Docker command . Scaling Corpora in this way will also multiply the number of Huey task workers you're able to run concurrently. You could alternatively scale Huey by increasing the number of Huey workers in a given Corpora container by setting the CRP_HUEY_WORKERS environment variable to something higher than the default, which is currently 10 .","title":"Managing"},{"location":"managing/#managing","text":"There are a few tasks, such as managing users, backing up and restoring corpus data, and installing plugins that Corpora admins may need to perform.","title":"Managing"},{"location":"managing/#managing-users","text":"To manage the users (or Scholars) on your instance of Corpora, click the blue \"Manage Scholars\" button in the footer of the table listing corpora on the homepage of the web application. Once on the \"Scholars\" page, you should see a table listing all the users in your instance of Corpora. You may click on the column names to sort, you can use the search box, or page through this list in order to find your user. Once you've found them, click on their username which should also be an orange link. This will bring up the \"Manage Scholar\" modal.","title":"Managing Users"},{"location":"managing/#grantingrevoking-admin-privileges","text":"Admins in Corpora are able to create a new corpus, view and edit all data for existing corpora, run all corpus notebooks, and run all tasks. You can make a user an admin for your instance of Corpora by clicking the orange \"Grant Admin Privileges\" button. Once a user is an admin, that same button will read \"Revoke Admin Privileges.\"","title":"Granting/Revoking Admin Privileges"},{"location":"managing/#grantingrevoking-corpus-permissions","text":"In order for a user to edit content, run a corpus notebook, or run certain tasks, they must either be an Admin for the instance of Corpora or be granted \"Editor\" privileges on a specific corpus. To do the latter, expand the \"Corpus permissions\" tray. In the \"New Permission\" form, provide the name of the corpus, choose \"Editor\" from the dropdown menu, and click \"Set Permission.\" To revoke that privilege, find it under \"Existing Permissions\" and choose \"None\" from the dropdown.","title":"Granting/Revoking Corpus Permissions"},{"location":"managing/#grantingrevoking-job-permissions","text":"While admins can run all jobs, users with Editor permissions on a corpus must be explicitly granted permission to run specific jobs. To do so, expand out the user's \"Job permissions\" tray. You can either check the box next to the specific task they need to be able to run, or check the \"Local (HUEY)\" box, which grants them privileges to run every local task. Once you've checked the appropriate box(es), click the orange \"Set Permissions\" button. Job permissions can be revoked by unchecking boxes and clicking \"Set Permissions\" again.","title":"Granting/Revoking Job Permissions"},{"location":"managing/#changing-passwords","text":"When logged in, individual users can change their passwords by clicking on the orange \"My Account\" button on the top right of a page in Corpora. Should they need their passwords reset, however, you may do so by pulling up their \"Manage Scholar\" modal and expanding the \"Change password\" tray. You may then provide their new password, confirm it, and click the \"Change Password\" button.","title":"Changing Passwords"},{"location":"managing/#backing-up-and-restoring-corpus-data","text":"When logged in as user with admin privileges, you should see several buttons on the footer of the table that lists all of the corpora on the home page of your instance of Corpora. To back up or restore corpus data, click the blue \"Corpus Backups\" button to manage corpus backups.","title":"Backing Up and Restoring Corpus Data"},{"location":"managing/#backing-up","text":"Corpus data can be backed up as a \"gzipped tar file,\" which includes metadata about your corpus, any files uploaded to your corpus or content types, and MongoDB database dumps of all the content in your corpus. To create an backup file, select the relevant corpus from the \"Corpus\" dropdown, provide a name (a default name is optionally provided for you), and click the orange \"Create\" button. Depending on how large your corpus is, this can take some time. Once an backup has been created, however, it should show up under \"Existing Backups\" beneath the creation form upon refreshing the page. Multiple backups for the same corpus may be created--they must, however, have unique names. In order to restore a corpus or migrate it to another instance of Corpora, you may download the file by clicking on its orange link in the \"Backup File\" column of the Existing Backups table. It is strongly recommended that you do not rename this file after you download it.","title":"Backing Up"},{"location":"managing/#restoring","text":"To restore a corpus backup, it must be listed in the \"Existing Backups\" table. If it's not listed there, scroll to the bottom of the page where it says \"Import a Backup File.\" You may simply drag and drop your downloaded backup file into the gray box to make it available.* Once your backup file appears in the \"Existing Backups\" table, you may click the blue \"Restore\" button to the right of it to commence the restore process. Note that the restore process maintains the original unique ID assigned to your corpus at the time of creation. As such, if you already have a version of that corpus on your instance of Corpora, you must first delete it before performing a restore. To delete an existing corpus you must be logged in as an admin user. You can then go to that corpus' home page, click on the \"Admin\" tab, scroll to the bottom, and use the corpus deletion form. Depending on the amount of data in your corpus, restoring can take a long time, as data is being restored to MongoDB, Elasticsearch, and Neo4J. * Note: Certain very large corpus backup files (usually over 3GB in size) may cause the upload process to timeout, preventing you from registering it as an available corpus to restore. If this happens, you will have to register it manually. To do so, navigate to the data directory of your instance of Corpora as it exists on the machine/server hosting your instance, i.e. /corpora/data . Inside that directory will be several subdirectories such as \"archive,\" \"link,\" \"search,\" etc. Go inside the subdirectory named \"corpora\" and find the directory named backups, i.e. /corpora/data/corpora/backups . Copy your large export file into this directory. You'll then have to execute a Django management command on the machine hosting Corpora by performing the following steps: Open a terminal or command prompt. Determine the name of the container running Corpora. You can do that by running the command docker ps . This will output information about all the containers running on your machine. In the \"NAMES\" column of this output, you'll want to find the container name for Corpora, which will either be something like corpora-corpora-1 if you're using Docker Compose, or corpora_corpora.1.[alphanumeric id] if you're using Docker Swarm. Make note of this container name. Execute the following command in your terminal: docker exec -it [container name] python3 manage.py register_backup_file [backup filename] Note that when providing the backup filename in the command above, you only need the name of the file (not the full path). Once that command executes and you get a message saying \"Backup file successfully registered,\" you should be able to see it listed as an available corpus backup to restore on the \"Corpus Backups\" page.","title":"Restoring"},{"location":"managing/#installing-plugins","text":"Corpora is designed with a plugin architecture that leverages Django's \"app\" convention. As such, installing plugins for Corpora is relatively easy, though you must have access to the filesystem of the server. A given plugin is ultimately a directory with a particular file structure (see creating plugins for Corpora ). To install a plugin, place its directory inside the \"plugins\" subdirectory living in the data directory for Corpora, i.e. /corpora/data/plugins . Once the plugin directory is copied there, you must also enable the plugin by adding it to the comma delimited list of enabled plugins stored in the CRP_INSTALLED_PLUGINS environment variable set for the Corpora container. The easiest way to do this is to edit the docker-compose.yml file, find the \"environment\" section of the \"corpora\" service, make sure the line specifying CRP_INSTALLED_PLUGINS isn't commented out, and add the name of the plugin directory to that variables' comma delimited value. Corpora must be restarted for it to be registered properly. And in order for containers to be aware of updated environment variables, the container must be stopped altogether and re-created by the Docker engine. To completely stop and restart Corpora while running with Docker Compose, issue these commands: # first change to your codebase directory with the docker-compose.yml file docker-compose down # wait 20 seconds or so for the containers to stop docker-compose up -d To completely stop and restart Corpora while running as a Swarm service, issue these commands: # first change to your codebase directory with the docker-compose.yml file docker stack rm corpora # wait 20 seconds or so for the containers to stop docker stack deploy corpora -c docker-compose.yml","title":"Installing Plugins"},{"location":"managing/#scaling-corpora","text":"Should your instance of Corpora receive high traffic volume, it is architected in such a way as to support multiple instances of the Corpora container--this is due to the fact that they all rely on the Redis container for session management. Scaling in this way has only been tested with a Docker Swarm deployment , and may be accomplished by setting the scale key in the service configuration for Corpora in docker-compose.yml , or by issuing the appropriate Docker command . Scaling Corpora in this way will also multiply the number of Huey task workers you're able to run concurrently. You could alternatively scale Huey by increasing the number of Huey workers in a given Corpora container by setting the CRP_HUEY_WORKERS environment variable to something higher than the default, which is currently 10 .","title":"Scaling Corpora"},{"location":"plugins/","text":"Plugins For Corpora Here is a list of plugins currently developed for Corpora, which will hopefully aid in creating new or repurposing existing ones. Document (built-in) CSV Maria Edgeworth Letters Project Tesseract Google Cloud Vision New Variorum Shakespeare Carlyle Letters Online Advanced Research Consortium For Wordpress These are custom Wordpress plugins that allow Wordpress sites to query Corpora in order to display a frontend interface. Generic Plugin for Corpora Maria Edgeworth Letters Project Carlyle Letters Online Advanced Research Consortium","title":"Plugins"},{"location":"plugins/#plugins","text":"","title":"Plugins"},{"location":"plugins/#for-corpora","text":"Here is a list of plugins currently developed for Corpora, which will hopefully aid in creating new or repurposing existing ones. Document (built-in) CSV Maria Edgeworth Letters Project Tesseract Google Cloud Vision New Variorum Shakespeare Carlyle Letters Online Advanced Research Consortium","title":"For Corpora"},{"location":"plugins/#for-wordpress","text":"These are custom Wordpress plugins that allow Wordpress sites to query Corpora in order to display a frontend interface. Generic Plugin for Corpora Maria Edgeworth Letters Project Carlyle Letters Online Advanced Research Consortium","title":"For Wordpress"},{"location":"rest_api/","text":"The Content REST API One of Corpora's affordances is a dynamically generated REST API for your corpus. At this time, the API is read-only. Once content types have been created within a corpus' \"Content Type Manager,\" and once instances of content exist for a given content type, that content type has its own API allowing third-party apps to query for content hosted within Corpora. There are two endpoints for any given content type--the \"List\" and \"Detail\" endpoints. If upon creating your corpus you chose the \"Open Access\" option, there is no authentication needed to access either of these endpoints. If you chose not to make your corpus open access, however, two things must be true before you can access these endpoints: You must provide the authentication token of a Corpora user within a header for each request. The header's name must be \"Authentication\" and the value for that header must be \"Token [ the authentication token ]\" You must be querying the endpoint from a valid IP address specified in the user profile associated with that authentication token. To add/manage valid IP addresses, simply login to Corpora and click the \"My Account\" button on the upper-right. The List Endpoint Content can be listed or queried for en-masse via the List endpoint, which can be accessed at the following URL: https://[ your.corpora.domain ]/api/corpus/[ corpus ID ]/[ content type name ]/ Your corpus ID can be determined by visiting your corpus' main page. For instance, if you created a corpus called \"My Corpus,\" you would click on your corpus' name (\"My Corpus\") on the main landing page at https://[ your.corpora.domain ]. Once you're on your corpus' main page, you'll note that the URL looks something like this: https://[ your.corpora.domain ]/corpus/5f60bf2cc879ea00329af449/ Your corpus' ID is located between the last two slashes of the URL (in the above example, the ID for the corpus is 5f60bf2cc879ea00329af449). The content type name is the name you provided for your content type when you created it in the Content Type Manager for your corpus. At present, all corpuses come with the \"Document\" content type by default. To access the List API for the Document content type in the example corpus above, for example, you'd use this endpoint: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Document/ List Endpoint Parameters This endpoint accepts several different GET parameters (passed via the query string): Parameter Purpose Example q To perform a general query against all keyword and text fields for your content [endpoint url]?q=search q_[field name] To perform a full-text query against a specific field [endpoint url]?q_title=Ulysses f_[field name] To filter by an exact value for a specific field [endpoint url]?f_color=green w_[field name] To perform wildcard matching on a specific field (note: if no asterix is found in the search term, one will be automatically appended at the end) [endpoint url]?w_name=Br* e_[field name] To specify only results that have a value (not empty) for a specific field (note: the value for this parameter is irrelevant) [endpoint url]?e_author=y r_[field name] To filter using a range of possible values (for number, decimal, date, and geospatial fields). Separate min and max values by \"to\" (for number, decimal, and date fields, if either min or max are omitted, range will just be \"less than or equal to\" or \"greater than or equal to\" respectively). When parsing dates for range queries, Corpora makes use of the dateutil package for Python so that dates can be specified in a variety of ways. Number/Decimal: [endpoint url]?r_size=6to10 Date: [endpoint url]?r_size=1/1/1980to12/31/1989 Geospatial (bounding box): [endpoint url]?r_location=[lon],[lat]to[lon],[lat] s_[field name] To sort results by field name, settings value to either \"ASC\" or \"DESC\". NOTE: geospatial and large text fields cannot be sorted. [endpoint_url]?s_pub_date=DESC a_terms_[aggregation_name] To produce a list of unique values for a field and their corresponding counts (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. [endpoint_url]?a_terms_uniquecolors=color a_min_[aggregation_name] To determine the min value for a field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. [endpoint_url]?a_min_lowestage=age a_max_[aggregation_name] To determine the max value for a field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. [endpoint_url]?a_max_highestage=age a_histogram_[aggregation_name] To produce a histogram of values at a given interval for a field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. The value for this parameter must be a field name, two underscores, and then the desired interval. [endpoint_url]?a_histogram_decades=age__10 a_geobounds_[aggregation_name] To produce a bounding box (top left and bottom right lat/long coordinates) for all the values in a geo_point field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. The value for this parameter must be a valid geo_point field name. [endpoint_url]?a_geobounds_region=coordinates a_geotile_[aggregation_name] To produce a series of \"geotiles\" and the corresponding number of values found within each tile for a given geo_point field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. The value for this parameter must be a field name, two underscores, and then the desired precision for the geotile. Read more about geotile aggregation and the precision value here . [endpoint_url]?a_geotile_areas=coordinates__9 page-size To specify the size of each page of results [endpoint_url]?page-size=50 page To specify which page of results you'd like [endpoint_url]?page=1 page-token After 9,000 records worth of pages, you'll receive a \"page token\" in the JSON response which will need to be captured and specified in order to retrieve further pages. [endpoint_url]?page-token=5f60bf2cc879ea00329af449 operator To specify which logical operator is used to combine queries (default \"and\") [endpoint_url]?q_color=red&q_holiday=Christmas&operator=or Chaining Parameters Parameters can of course be chained together. If you wanted, for instance, to see the first 50 Documents with \"Ulysses\" in the title sorted by publication date in descending order, you could query the endpoint like this: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Document/?q_title=Ulysses&s_pub_date=DESC&page-size=50&page=1 You may perform queries using multiple search terms on the same field, though this is not acheived by chaining the same GET parameter together. In order to do this, you must separate the multiple values with two underscores (__) as your delimiter. So, to search a hypothetical field named \"color\" for both \"red\" and \"green\" values, do this: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_color=red__green By default, however, the \"and\" operator is used to combine queries, so the above query would only make sense in a scenario where the color field is multi-valued. If you want to change the operator used to combine queries, you can do so by using the operator parameter like so: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_color=red__green&operator=or This would provide results where the hypothetical color field contains the values red or green . Note : when changing the operator in this manner, you're changing how all queries are combined. Consider this scenario: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_color=red__green&q_texture=smooth&operator=or Because the operator is changed to or , and unfortunate side effect occurs: results are returned where the value of color is either red or green or the value of the hypothetical texture field is smooth . In other words, you could have results where the value of texture is smooth but the value of color is brown ! In order to construct queries with more complicated, nested boolean logic, you may make use of numerical prefixes that group queries together. If, for instance, you wanted results where, effectively (texture=smooth AND (color=red OR color=green)), you could create the following query: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_texture=smooth&1_q_color=red__green&1_operator=or The prefix of 1_ before the q_color and operator parameters place them in a nested group together. That nested group is then combined with the q_texture query using the default and operator. The Corpora list API supports up to 9 different groups to create complicated nested queries, making use of numerical prefixes 1_ through 9_ . Endpoint Output Results are returned in JSON format, with two main (upper-level) keys: \"meta,\" and \"records.\" The \"meta\" key is a hash with the following key/value pairs: content_type : The name of the content type being queried, i.e. \"Document\" has_next_page : A boolean specifying whether more pages of results exist, i.e. true num_pages : The total number of pages available given the specified page size, i.e. 122 page : The current page of results, i.e. 1 page_size : The size of each page of results, i.e. 50 total : The total number of documents matching query parameters, i.e. 6,097 The \"records\" key refers to a list of actual results (the content being queried for). Each item in the list is a hash representing the content, with the keys being field names and the values being the values stored in those fields. NOTE : Aside from the mandatory \"id,\" \"label,\" and \"uri\" fields, only fields for which the \"In Lists?\" flag has been set to true in the Content Type Manager appear here. The Detail Endpoint Whereas the List endpoint provides a way to query for content and see values for fields marked as being \"In Lists,\" the detail endpoint allows you to see the values for every field for a given, individual piece of content. To access the endpoint for an individual piece of content, use this URL: https://[ your.corpora.domain ]/api/corpus/[ corpus ID ]/[ content type name ]/[ content ID ]/ So, for instance, assuming you're interested in all the data for a Document with the ID \"5f734833741449002ba9907e,\" you could access that data at the following URL: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Document/5f734833741449002ba9907e/ Results are returned in JSON format, as a hash where keys are field names and values are the data stored in those fields.","title":"REST API"},{"location":"rest_api/#the-content-rest-api","text":"One of Corpora's affordances is a dynamically generated REST API for your corpus. At this time, the API is read-only. Once content types have been created within a corpus' \"Content Type Manager,\" and once instances of content exist for a given content type, that content type has its own API allowing third-party apps to query for content hosted within Corpora. There are two endpoints for any given content type--the \"List\" and \"Detail\" endpoints. If upon creating your corpus you chose the \"Open Access\" option, there is no authentication needed to access either of these endpoints. If you chose not to make your corpus open access, however, two things must be true before you can access these endpoints: You must provide the authentication token of a Corpora user within a header for each request. The header's name must be \"Authentication\" and the value for that header must be \"Token [ the authentication token ]\" You must be querying the endpoint from a valid IP address specified in the user profile associated with that authentication token. To add/manage valid IP addresses, simply login to Corpora and click the \"My Account\" button on the upper-right.","title":"The Content REST API"},{"location":"rest_api/#the-list-endpoint","text":"Content can be listed or queried for en-masse via the List endpoint, which can be accessed at the following URL: https://[ your.corpora.domain ]/api/corpus/[ corpus ID ]/[ content type name ]/ Your corpus ID can be determined by visiting your corpus' main page. For instance, if you created a corpus called \"My Corpus,\" you would click on your corpus' name (\"My Corpus\") on the main landing page at https://[ your.corpora.domain ]. Once you're on your corpus' main page, you'll note that the URL looks something like this: https://[ your.corpora.domain ]/corpus/5f60bf2cc879ea00329af449/ Your corpus' ID is located between the last two slashes of the URL (in the above example, the ID for the corpus is 5f60bf2cc879ea00329af449). The content type name is the name you provided for your content type when you created it in the Content Type Manager for your corpus. At present, all corpuses come with the \"Document\" content type by default. To access the List API for the Document content type in the example corpus above, for example, you'd use this endpoint: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Document/","title":"The List Endpoint"},{"location":"rest_api/#list-endpoint-parameters","text":"This endpoint accepts several different GET parameters (passed via the query string): Parameter Purpose Example q To perform a general query against all keyword and text fields for your content [endpoint url]?q=search q_[field name] To perform a full-text query against a specific field [endpoint url]?q_title=Ulysses f_[field name] To filter by an exact value for a specific field [endpoint url]?f_color=green w_[field name] To perform wildcard matching on a specific field (note: if no asterix is found in the search term, one will be automatically appended at the end) [endpoint url]?w_name=Br* e_[field name] To specify only results that have a value (not empty) for a specific field (note: the value for this parameter is irrelevant) [endpoint url]?e_author=y r_[field name] To filter using a range of possible values (for number, decimal, date, and geospatial fields). Separate min and max values by \"to\" (for number, decimal, and date fields, if either min or max are omitted, range will just be \"less than or equal to\" or \"greater than or equal to\" respectively). When parsing dates for range queries, Corpora makes use of the dateutil package for Python so that dates can be specified in a variety of ways. Number/Decimal: [endpoint url]?r_size=6to10 Date: [endpoint url]?r_size=1/1/1980to12/31/1989 Geospatial (bounding box): [endpoint url]?r_location=[lon],[lat]to[lon],[lat] s_[field name] To sort results by field name, settings value to either \"ASC\" or \"DESC\". NOTE: geospatial and large text fields cannot be sorted. [endpoint_url]?s_pub_date=DESC a_terms_[aggregation_name] To produce a list of unique values for a field and their corresponding counts (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. [endpoint_url]?a_terms_uniquecolors=color a_min_[aggregation_name] To determine the min value for a field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. [endpoint_url]?a_min_lowestage=age a_max_[aggregation_name] To determine the max value for a field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. [endpoint_url]?a_max_highestage=age a_histogram_[aggregation_name] To produce a histogram of values at a given interval for a field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. The value for this parameter must be a field name, two underscores, and then the desired interval. [endpoint_url]?a_histogram_decades=age__10 a_geobounds_[aggregation_name] To produce a bounding box (top left and bottom right lat/long coordinates) for all the values in a geo_point field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. The value for this parameter must be a valid geo_point field name. [endpoint_url]?a_geobounds_region=coordinates a_geotile_[aggregation_name] To produce a series of \"geotiles\" and the corresponding number of values found within each tile for a given geo_point field (appears in the \"meta\" section of results). Any alphanumeric string may be used for [aggregation_name]. The value for this parameter must be a field name, two underscores, and then the desired precision for the geotile. Read more about geotile aggregation and the precision value here . [endpoint_url]?a_geotile_areas=coordinates__9 page-size To specify the size of each page of results [endpoint_url]?page-size=50 page To specify which page of results you'd like [endpoint_url]?page=1 page-token After 9,000 records worth of pages, you'll receive a \"page token\" in the JSON response which will need to be captured and specified in order to retrieve further pages. [endpoint_url]?page-token=5f60bf2cc879ea00329af449 operator To specify which logical operator is used to combine queries (default \"and\") [endpoint_url]?q_color=red&q_holiday=Christmas&operator=or","title":"List Endpoint Parameters"},{"location":"rest_api/#chaining-parameters","text":"Parameters can of course be chained together. If you wanted, for instance, to see the first 50 Documents with \"Ulysses\" in the title sorted by publication date in descending order, you could query the endpoint like this: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Document/?q_title=Ulysses&s_pub_date=DESC&page-size=50&page=1 You may perform queries using multiple search terms on the same field, though this is not acheived by chaining the same GET parameter together. In order to do this, you must separate the multiple values with two underscores (__) as your delimiter. So, to search a hypothetical field named \"color\" for both \"red\" and \"green\" values, do this: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_color=red__green By default, however, the \"and\" operator is used to combine queries, so the above query would only make sense in a scenario where the color field is multi-valued. If you want to change the operator used to combine queries, you can do so by using the operator parameter like so: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_color=red__green&operator=or This would provide results where the hypothetical color field contains the values red or green . Note : when changing the operator in this manner, you're changing how all queries are combined. Consider this scenario: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_color=red__green&q_texture=smooth&operator=or Because the operator is changed to or , and unfortunate side effect occurs: results are returned where the value of color is either red or green or the value of the hypothetical texture field is smooth . In other words, you could have results where the value of texture is smooth but the value of color is brown ! In order to construct queries with more complicated, nested boolean logic, you may make use of numerical prefixes that group queries together. If, for instance, you wanted results where, effectively (texture=smooth AND (color=red OR color=green)), you could create the following query: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Clothing/?q_texture=smooth&1_q_color=red__green&1_operator=or The prefix of 1_ before the q_color and operator parameters place them in a nested group together. That nested group is then combined with the q_texture query using the default and operator. The Corpora list API supports up to 9 different groups to create complicated nested queries, making use of numerical prefixes 1_ through 9_ .","title":"Chaining Parameters"},{"location":"rest_api/#endpoint-output","text":"Results are returned in JSON format, with two main (upper-level) keys: \"meta,\" and \"records.\" The \"meta\" key is a hash with the following key/value pairs: content_type : The name of the content type being queried, i.e. \"Document\" has_next_page : A boolean specifying whether more pages of results exist, i.e. true num_pages : The total number of pages available given the specified page size, i.e. 122 page : The current page of results, i.e. 1 page_size : The size of each page of results, i.e. 50 total : The total number of documents matching query parameters, i.e. 6,097 The \"records\" key refers to a list of actual results (the content being queried for). Each item in the list is a hash representing the content, with the keys being field names and the values being the values stored in those fields. NOTE : Aside from the mandatory \"id,\" \"label,\" and \"uri\" fields, only fields for which the \"In Lists?\" flag has been set to true in the Content Type Manager appear here.","title":"Endpoint Output"},{"location":"rest_api/#the-detail-endpoint","text":"Whereas the List endpoint provides a way to query for content and see values for fields marked as being \"In Lists,\" the detail endpoint allows you to see the values for every field for a given, individual piece of content. To access the endpoint for an individual piece of content, use this URL: https://[ your.corpora.domain ]/api/corpus/[ corpus ID ]/[ content type name ]/[ content ID ]/ So, for instance, assuming you're interested in all the data for a Document with the ID \"5f734833741449002ba9907e,\" you could access that data at the following URL: https://[ your.corpora.domain ]/api/corpus/5f60bf2cc879ea00329af449/Document/5f734833741449002ba9907e/ Results are returned in JSON format, as a hash where keys are field names and values are the data stored in those fields.","title":"The Detail Endpoint"},{"location":"assets/plugins/","text":"Plugins Several plugins have been developed for use with Corpora, and some of them are listed below. They are listed here in order to both allow others to use them as well and also as a reference for developing new ones. For more explicit documentation on building custom plugins, see here . For instructions on how to deploy a plugin on your own instance of Corpora, see here The Document Plugin The Document plugin is the only plugin Corpora comes bundled with. When creating a new corpus, users are prompted with the option to include the content types that come built-in with the Document plugin. These content types (and the custom functionality baked into them) are intended to provide users with a generic data structure capable of efficiently storing, displaying, and transcribing page-based documents. The Document plugin also integrates with both the Tesseract plugin and the Google Cloud Vision plugin to enable the running of OCR jobs using Corpora's asynchronous task queue.","title":"Plugins"},{"location":"assets/plugins/#plugins","text":"Several plugins have been developed for use with Corpora, and some of them are listed below. They are listed here in order to both allow others to use them as well and also as a reference for developing new ones. For more explicit documentation on building custom plugins, see here . For instructions on how to deploy a plugin on your own instance of Corpora, see here","title":"Plugins"},{"location":"assets/plugins/#the-document-plugin","text":"The Document plugin is the only plugin Corpora comes bundled with. When creating a new corpus, users are prompted with the option to include the content types that come built-in with the Document plugin. These content types (and the custom functionality baked into them) are intended to provide users with a generic data structure capable of efficiently storing, displaying, and transcribing page-based documents. The Document plugin also integrates with both the Tesseract plugin and the Google Cloud Vision plugin to enable the running of OCR jobs using Corpora's asynchronous task queue.","title":"The Document Plugin"}]}